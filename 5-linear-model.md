---
marp: true
style: |
    section {
        font-size: 20px;
    }
paginate: true
header: Линейная регрессия
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: title -->

# **Линейная регрессия: полное руководство**

### От постановки задачи до регуляризации и анализа допущений

---

# Что такое регрессия?

## Задача прогнозирования: Регрессия vs Классификация

**Регрессия**: Прогнозирование непрерывной числовой величины (цена, температура, спрос)

**Классификация**: Прогнозирование дискретной метки/категории (спам/не спам, кошка/собака)

---

# Постановка задачи линейной регрессии

## Формальная постановка задачи

- **Дано**: Признаковые описания объектов: $X = (x_1^{(i)}, x_2^{(i)}, ..., x_m^{(i)})$, $i=1..n$
- **Найти**: Вещественный целевой вектор $y = (y^{(1)}, y^{(2)}, ..., y^{(n)})$
- **Цель**: Найти $f(X)$ такую, что $f(X^{(i)}) \approx y^{(i)}$
- **Гипотеза**: Линейная: $f(X) = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m$
- **Векторная форма**: $f(X) = \langle W, X \rangle + w_0$, где $W = (w_1, w_2, ..., w_m)$

![bg right:40% w:500](
https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

---

# Как измерить качество? Метрики регрессии (Часть 1)

## Ошибка предсказания: Функция потерь (Loss)

- **Остаток (Residual)**: $e_i = y_i - \hat{y_i}$
- **MSE (Mean Squared Error)**: $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$
  - Сурово наказывает за большие ошибки
  - Дифференцируема
- **MAE (Mean Absolute Error)**: $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y_i}|$
  - Менее чувствительна к выбросам

---

# Метрики регрессии (Часть 2)

- **Различные модификаци над MAE**: MAE=600 - хорошо или плохо? *всё относительно.*

  * MAPE (Mean Average Percentage Error)
$$ MAPE=\sum_i\frac{|y_{true_{i}} - y_{pred_{i}}|}{y_{true_{i}}}$$

    Недостаток - неусточивость относительно деления на 0 либо маленькие величины (получим большие проценты ошибки? не обязательно желаемые)
  
  * WAPE (Weighted Average Percentage Error)

    $$WAPE=\frac{\sum_i|y_{true_{i}} - y_{pred_{i}}|}{\sum_i{y_{true_{i}}}}$$
    * По-простому, это разница площадей графиков под кривыми $y_{pred}$ и $y_{true}$, относительно $y_{true}$
  * Как будет выглядеть формула, если у нас некоторые дни наблюдений имеют большую важность по сравнению с другими?
  * **Какой есть нюанс в использовании указанных percentage метрик?**

---
![alt text](image.png)

Вариант лечения - SMAPE (вникнуть самостоятельно)

---

# Цель — минимизация ошибки

## Задача оптимизации

Задача линейной регрессии сводится к поиску вектора весов $W$, который минимизирует функцию среднеквадратичной ошибки (MSE)

**Целевая функция**: $L(W) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \langle W, X_i \rangle)^2 \rightarrow \min_{W}$

---

# Способы решения: Аналитический метод (Normal Equation)

## Точное решение "в лоб"

- **Матричная форма**: $Y = XW + \epsilon$
- **Решение**: $\nabla_W L(W) = 0$
- **Нормальное уравнение**: $W = (X^T X)^{-1} X^T Y$

**Плюсы**: Точное решение (если оно существует)

**Минусы**: Вычислительно дорогое ($O(n^3)$), требует обратимости матрицы $X^T X$

---

# Способы решения: Градиентный спуск (Идея)

## Итеративный метод: Градиентный спуск

- **Аналогия**: Шарик скатывается вниз по склону холма
- **Градиент**: Вектор, указывающий направление наискорейшего роста функции
- **Шаг**: $W_{new} = W_{old} - \eta \cdot \nabla_W L(W_{old})$, где $\eta$ — скорость обучения

---

# Градиентный спуск (Детали и виды)

## Практические аспекты градиентного спуска

- **Скорость обучения ($\eta$)**:
  - Слишком малая → медленная сходимость
  - Слишком большая → перескоки минимума, расходимость

**Виды**:
- **Batch GD**: Градиент по всей выборке (точно, но медленно)
- **Stochastic GD (SGD)**: Градиент по одному случайному объекту (быстро, но noisy)
- **Mini-batch GD**: Градиент по небольшой подвыборке (компромисс)

![bg right:35% w:400](
https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/512px-Gradient_descent.svg.png?20120807190453
)

---

# Взгляд с другой стороны: Вероятностная модель

## Почему именно MSE? Вероятностное обоснование

- **Модель**: $y_i = \langle W, X_i \rangle + \epsilon_i$, где $\epsilon_i$ — ошибка
- **Предположения**: 
  - $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ (ошибки распределены нормально)
  - $\sigma - const$, 
  - признаки линейно объясняют y
  - признаки попарно независимы

---

# Метод максимального правдоподобия (ММП)

## Найдем правдоподобные параметры

- **Правдоподобие (Likelihood)**: $L(W) = P(Data | W) = \prod_{i=1}^{n} P(y_i | X_i, W)$
(использовали попарную независимость признаков)
- **Цель ММП**: Найти такие $W$, которые максимизируют $L(W)$
- **Вывод**: $L(W) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \exp(-\frac{(y_i - \langle W, X_i \rangle)^2}{2\sigma^2})$
(исходит из вероятностного предположения нормальной распределенности остатков и одинаковой дисперсии, иначе не получится такая формула)
- **Логарифмическое правдоподобие**: $\log L(W) = const - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i - \langle W, X_i \rangle)^2$
- **Ключевой вывод**: Максимизация $\log L(W)$ эквивалентна минимизации $\sum (y_i - \hat{y_i})^2 = MSE$
- **Минимизация MSE является оптимальной** при условии нормальности и независимости ошибок

---

# Проблема 1: Мультиколлинеарность

## Что если признаки коррелируют?

**Определение**: Наличие сильной линейной зависимости между признаками

**Проблемы**:
- Неустойчивость оценок весов
- Затрудненная интерпретация
- Матрица $X^T X$ становится плохо обусловленной

**Диагностика**: VIF (Variance Inflation Factor), корреляционные матрицы

---

# Проблема 2: Гетероскедастичность

## Непостоянная дисперсия ошибок

**Нарушение**: $\epsilon_i \sim \mathcal{N}(0, \sigma_i^2)$, а не $\mathcal{N}(0, \sigma^2)$

**Последствия**:
- Веса остаются несмещенными, но теряют эффективность
- Стандартные ошибки становятся неверными

**Диагностика**: График остатков ($e_i$) vs предсказанных значений ($\hat{y_i}$)

**Лечение**: Взвешенная регрессия, логарифмирование
![bg right:40% w:500](
https://upload.wikimedia.org/wikipedia/commons/a/a5/Heteroscedasticity.png)

---

# Проблема 3: Не нормальное распределение ошибок

## Пример: Ошибки, распределенные по Лапласу

- **Предположение**: $\epsilon_i \sim Laplace(0, b)$
- **Функция правдоподобия**: $L(W) \propto \prod \exp(-\frac{|y_i - \langle W, X_i \rangle|}{b})$
- **ММП оценка**: Максимизация правдоподобия эквивалентна минимизации $\sum |y_i - \hat{y_i}|$ (MAE)
- **Вывод**: При выбросах такого типа максимальному правдоподобию будет соответствовать регрессия, минимизирующая MAE, а не MSE

---

# Проблема 4: Нелинейность зависимости

## Что если мир не линейный?

**Решение**: Преобразование признаков

**Методы**:
- Полиномиальная регрессия
- Добавление бинарных признаков
- Использование сплайнов

**Итог**: Почти любую зависимость можно свести к линейной модели в пространстве признаков более высокой размерности
![bg right:35% w:400](
https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyHkcnQcqBu-GYtVbFaocaV33_bW4EyLe7Jw&s)

---

# Переобучение и нестабильность

## Мотивация: Борьба с переобучением

- **Переобучение**: Модель идеально описывает обучающие данные, но плохо обобщается
- **Причины**: Слишком сложная модель, много признаков, мультиколлинеарность
- **Решение**: Регуляризация — добавление штрафа за сложность модели

![bg right:35% w:400](
https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg)

---

# Ridge Regression (L2-регуляризация)

## Контроль величины весов

- **Функция потерь**: $L_{ridge}(W) = MSE(W) + \lambda \sum_{j=1}^{m} w_j^2$
- **$\lambda$** — гиперпараметр, сила регуляризации
- **Эффект**: "Ужимает" веса, но не обнуляет
- **Решение**: $W_{ridge} = (X^T X + \lambda I)^{-1} X^T Y$

---

# Lasso Regression (L1-регуляризация)

## Отбор признаков

- **Функция потерь**: $L_{lasso}(W) = MSE(W) + \lambda \sum_{j=1}^{m} |w_j|$
- **Эффект**: Обнуляет веса неважных признаков
- **Плюс**: Интерпретируемая модель с малым подмножеством признаков

<br>
<br>

*это спрашивают на собеседованиях!*
Самостоятельно выяснить (строго математически, линии уровня - не доказательство), почему так происходит, и как обнуление отдельного признака связано с параметром $\lambda$

---

# ElasticNet

## Компромисс между Ridge и Lasso

- **Функция потерь**: $L_{enet}(W) = MSE(W) + \lambda_1 \sum |w_j| + \lambda_2 \sum w_j^2$
- **Применение**: Полезно когда признаков очень много и они сильно коррелируют

---

# Вероятностный смысл регуляризации

## Регуляризация как байесовский подход

- **ММП**: $W_{ММП} = \arg\max_W P(Data | W)$
- **MAP**: $W_{MAP} = \arg\max_W P(W | Data) = \arg\max_W P(Data | W) \cdot P(W)$
- **Априорное распределение $P(W)$**:
  - **Ridge (L2)**: $w_j \sim \mathcal{N}(0, \frac{1}{\lambda})$
  - **Lasso (L1)**: $w_j \sim Laplace(0, \frac{1}{\lambda})$

---

# Плюсы линейной регрессии

## Сильные стороны

- 🟢 **Интерпретируемость**: Веса имеют четкий смысл
- 🟢 **Вычислительная эффективность**: Быстрое обучение и предсказание
- 🟢 **Надежность**: Хорошо изучена, есть строгая статистическая теория
- 🟢 **Простота**: Легко реализовать и использовать как baseline

---

# Минусы линейной регрессии

## Ограничения и слабые стороны

- 🔴 **Чувствительность к выбросам** (из-за MSE)
- 🔴 **Строгие допущения** (линейность, нормальность, гомоскедастичность)
- 🔴 **Не может моделировать сложные нелинейные зависимости** без feature engineering
- 🔴 **Чувствительность к мультиколлинеарности** (без регуляризации)

---

# Этапы построения модели на практике

## Практический пайплайн

1. EDA
2. Предобработка: обработка пропусков, кодирование категорий, масштабирование
3. Проверка на мультиколлинеарность (VIF)
4. Обучение модели, подбор гиперпараметра $\lambda$ (кросс-валидация)
5. Анализ ошибок (проверка допущений)
6. Интерпретация результатов

---

# Кросс-валидация и подбор гиперпараметров

## Как выбрать lambda?

- **Цель**: Найти значение $\lambda$, дающее лучшую обобщающую способность
- **Метод**: k-Fold Cross-Validation
- **Процесс**: Разбиваем данные на k частей. k раз обучаем на (k-1) частях и валидируем на тестовых данных
- **Критерий**: Минимизация MSE на валидационных фолдах

---

# Сравнение методов регуляризации

## Ridge vs Lasso vs ElasticNet

| Критерий | Ridge (L2) | Lasso (L1) | ElasticNet |
|----------|------------|------------|------------|
| Отбор признаков | Нет | Да | Да |
| Коррелир. признаки | Сохраняет все | Оставляет один | Сохраняет группу |
| Интерпретация | Сложнее | Проще | Сложнее |
| Когда использовать? | Много полезных признаков | Много бесполезных признаков | Сильная корреляция + отбор |

---

# Резюме ключевых идей

## Главное, что нужно запомнить

- Минимизация MSE = ММП при нормальности ошибок
- Нарушение допущений ведет к некорректным выводам
- Регуляризация — мощный инструмент борьбы с переобучением
- Lasso отбирает признаки, Ridge — ужимает веса
- Всегда проверяйте остатки!

---

# Что дальше?

## Развитие темы

- **Обобщенная линейная модель (GLM)**: Для не-normal данных
- **Непараметрическая регрессия**: Ядерная регрессия, сплайны
- **Ансамбли на деревьях**: xgBoost, LightGBM
- **Нейронные сети**

---

<!-- _class: title -->
![bg](https://images.unsplash.com/photo-1533750349088-cd871a92f312?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)

# **Спасибо за внимание!**

## Вопросы?

**Контакты**: Ваша почта / Telegram

---

# Источники и литература

## Литература

1. Hastie, Tibshirani, Friedman "The Elements of Statistical Learning"
2. James, Witten, Hastie, Tibshirani "An Introduction to Statistical Learning"
3. Andrew Ng "Machine Learning" (Coursera)
4. Sebastian Raschka "Python Machine Learning"
5. Документация sklearn.linear_model