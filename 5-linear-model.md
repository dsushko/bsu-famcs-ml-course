---
marp: true
theme: |
    section {
        font-family: 'Arial', sans-serif;
        font-size: 20px;
    }
paginate: true
header: Линейная регрессия
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: title -->
![bg left:40%](https://miro.medium.com/v2/resize:fit:1400/1*T3o2qV7X7R0rZfypOmXW6g.png)

# **Линейная регрессия: полное руководство**

### От постановки задачи до регуляризации и анализа допущений

---

# Что такое регрессия?

## Задача прогнозирования: Регрессия vs Классификация

**Регрессия**: Прогнозирование непрерывной числовой величины (цена, температура, спрос)

**Классификация**: Прогнозирование дискретной метки/категории (спам/не спам, кошка/собака)

![w:450](https://i.stack.imgur.com/1h9V2.png) ![w:450](https://miro.medium.com/v2/resize:fit:1400/1*Z7sV2kPezVt1_E_dk4ycAQ.png)

---

# Постановка задачи линейной регрессии

## Формальная постановка задачи

- **Дано**: Признаковые описания объектов: $X = (x_1^{(i)}, x_2^{(i)}, ..., x_m^{(i)})$, $i=1..n$
- **Найти**: Вещественный целевой вектор $y = (y^{(1)}, y^{(2)}, ..., y^{(n)})$
- **Цель**: Найти $f(X)$ такую, что $f(X^{(i)}) \approx y^{(i)}$
- **Гипотеза**: Линейная: $f(X) = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m$
- **Векторная форма**: $f(X) = \langle W, X \rangle + w_0$, где $W = (w_1, w_2, ..., w_m)$

---

# Как измерить качество? Метрики регрессии (Часть 1)

## Ошибка предсказания: Функция потерь (Loss)

- **Остаток (Residual)**: $e_i = y_i - \hat{y_i}$
- **MSE (Mean Squared Error)**: $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$
  - Сурово наказывает за большие ошибки
  - Дифференцируема
- **MAE (Mean Absolute Error)**: $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y_i}|$
  - Менее чувствительна к выбросам

---

# Метрики регрессии (Часть 2)

## Метрики для интерпретации

- **$R^2$ (R-квадрат)**: Доля дисперсии целевой переменной, объясненной моделью
  - $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$
  - Лучшая модель → $R^2$ ближе к 1
- **Adjusted $R^2$**: Корректирует $R^2$ с учетом количества признаков
- **RMSLE**: Полезно, когда важен порядок величины, а не абсолютное значение

---

# Цель — минимизация ошибки

## Задача оптимизации

Задача линейной регрессии сводится к поиску вектора весов $W$, который минимизирует функцию среднеквадратичной ошибки (MSE)

**Целевая функция**: $L(W) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \langle W, X_i \rangle)^2 \rightarrow \min_{W}$

---

# Способы решения: Аналитический метод (Normal Equation)

## Точное решение "в лоб"

- **Матричная форма**: $Y = XW + \epsilon$
- **Решение**: $\nabla_W L(W) = 0$
- **Нормальное уравнение**: $W = (X^T X)^{-1} X^T Y$

**Плюсы**: Точное решение (если оно существует)

**Минусы**: Вычислительно дорогое ($O(n^3)$), требует обратимости матрицы $X^T X$

---

# Способы решения: Градиентный спуск (Идея)

## Итеративный метод: Градиентный спуск

- **Аналогия**: Шарик скатывается вниз по склону холма
- **Градиент**: Вектор, указывающий направление наискорейшего роста функции
- **Шаг**: $W_{new} = W_{old} - \eta \cdot \nabla_W L(W_{old})$, где $\eta$ — скорость обучения

![bg right:40%](https://miro.medium.com/v2/resize:fit:1400/1*NCA8_2LqwV_27l7U6N_PAA.gif)

---

# Градиентный спуск (Детали и виды)

## Практические аспекты градиентного спуска

- **Скорость обучения ($\eta$)**:
  - Слишком малая → медленная сходимость
  - Слишком большая → перескоки минимума, расходимость

**Виды**:
- **Batch GD**: Градиент по всей выборке (точно, но медленно)
- **Stochastic GD (SGD)**: Градиент по одному случайному объекту (быстро, но noisy)
- **Mini-batch GD**: Градиент по небольшой подвыборке (компромисс)

---

# Взгляд с другой стороны: Вероятностная модель

## Почему именно MSE? Вероятностное обоснование

- **Модель**: $y_i = \langle W, X_i \rangle + \epsilon_i$, где $\epsilon_i$ — ошибка
- **Предположение**: $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ (ошибки распределены нормально)
- **Следствие**: $y_i | X_i \sim \mathcal{N}(\langle W, X_i \rangle, \sigma^2)$

---

# Метод максимального правдоподобия (ММП)

## Найдем最правдоподобные параметры

- **Правдоподобие (Likelihood)**: $L(W) = P(Data | W) = \prod_{i=1}^{n} P(y_i | X_i, W)$
- **Цель ММП**: Найти такие $W$, которые максимизируют $L(W)$
- **Вывод**: $L(W) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \exp(-\frac{(y_i - \langle W, X_i \rangle)^2}{2\sigma^2})$

---

# Связь ММП и MSE

## Максимизация правдоподобия = Минимизация MSE

- **Логарифмическое правдоподобие**: $\log L(W) = const - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i - \langle W, X_i \rangle)^2$
- **Ключевой вывод**: Максимизация $\log L(W)$ эквивалентна минимизации $\sum (y_i - \hat{y_i})^2$
- **Минимизация MSE является оптимальной** при условии нормальности и независимости ошибок

---

# Проблема 1: Мультиколлинеарность

## Что если признаки коррелируют?

**Определение**: Наличие сильной линейной зависимости между признаками

**Проблемы**:
- Неустойчивость оценок весов
- Затрудненная интерпретация
- Матрица $X^T X$ становится плохо обусловленной

**Диагностика**: VIF (Variance Inflation Factor), корреляционные матрицы

---

# Проблема 2: Гетероскедастичность

## Непостоянная дисперсия ошибок

**Нарушение**: $\epsilon_i \sim \mathcal{N}(0, \sigma_i^2)$, а не $\mathcal{N}(0, \sigma^2)$

**Последствия**:
- Веса остаются несмещенными, но теряют эффективность
- Стандартные ошибки становятся неверными

**Диагностика**: График остатков ($e_i$) vs предсказанных значений ($\hat{y_i}$)

**Лечение**: Взвешенная регрессия, логарифмирование

---

# Проблема 3: Не нормальное распределение ошибок

## Пример: Ошибки, распределенные по Лапласу

- **Предположение**: $\epsilon_i \sim Laplace(0, b)$
- **Функция правдоподобия**: $L(W) \propto \prod \exp(-\frac{|y_i - \langle W, X_i \rangle|}{b})$
- **ММП оценка**: Максимизация правдоподобия эквивалентна минимизации $\sum |y_i - \hat{y_i}|$ (MAE)
- **Вывод**: При выбросах более робастной будет регрессия, минимизирующая MAE

---

# Проблема 4: Нелинейность зависимости

## Что если мир не линейный?

**Решение**: Преобразование признаков

**Методы**:
- Полиномиальная регрессия
- Добавление бинарных признаков
- Использование сплайнов

**Итог**: Почти любую зависимость можно свести к линейной модели в пространстве признаков более высокой размерности

---

# Переобучение и нестабильность

## Мотивация: Борьба с переобучением

- **Переобучение**: Модель идеально описывает обучающие данные, но плохо обобщается
- **Причины**: Слишком сложная модель, много признаков, мультиколлинеарность
- **Решение**: Регуляризация — добавление штрафа за сложность модели

---

# Ridge Regression (L2-регуляризация)

## Контроль величины весов

- **Функция потерь**: $L_{ridge}(W) = MSE(W) + \lambda \sum_{j=1}^{m} w_j^2$
- **$\lambda$** — гиперпараметр, сила регуляризации
- **Эффект**: "Ужимает" веса, но не обнуляет
- **Решение**: $W_{ridge} = (X^T X + \lambda I)^{-1} X^T Y$

---

# Lasso Regression (L1-регуляризация)

## Отбор признаков

- **Функция потерь**: $L_{lasso}(W) = MSE(W) + \lambda \sum_{j=1}^{m} |w_j|$
- **Эффект**: Обнуляет веса неважных признаков
- **Плюс**: Интерпретируемая модель с малым подмножеством признаков

---

# ElasticNet

## Компромисс между Ridge и Lasso

- **Функция потерь**: $L_{enet}(W) = MSE(W) + \lambda_1 \sum |w_j| + \lambda_2 \sum w_j^2$
- **Применение**: Полезно когда признаков очень много и они сильно коррелируют

---

# Вероятностный смысл регуляризации

## Регуляризация как байесовский подход

- **ММП**: $W_{ММП} = \arg\max_W P(Data | W)$
- **MAP**: $W_{MAP} = \arg\max_W P(W | Data) = \arg\max_W P(Data | W) \cdot P(W)$
- **Априорное распределение $P(W)$**:
  - **Ridge (L2)**: $w_j \sim \mathcal{N}(0, \frac{1}{\lambda})$
  - **Lasso (L1)**: $w_j \sim Laplace(0, \frac{1}{\lambda})$

---

# Плюсы линейной регрессии

## Сильные стороны

- 🟢 **Интерпретируемость**: Веса имеют четкий смысл
- 🟢 **Вычислительная эффективность**: Быстрое обучение и предсказание
- 🟢 **Надежность**: Хорошо изучена, есть строгая статистическая теория
- 🟢 **Простота**: Легко реализовать и использовать как baseline

---

# Минусы линейной регрессии

## Ограничения и слабые стороны

- 🔴 **Чувствительность к выбросам** (из-за MSE)
- 🔴 **Строгие допущения** (линейность, нормальность, гомоскедастичность)
- 🔴 **Не может моделировать сложные нелинейные зависимости** без feature engineering
- 🔴 **Чувствительность к мультиколлинеарности** (без регуляризации)

---

# Этапы построения модели на практике

## Практический пайплайн

1. Разведочный анализ данных (EDA)
2. Предобработка: обработка пропусков, кодирование категорий, масштабирование
3. Проверка на мультиколлинеарность (VIF)
4. Обучение модели, подбор гиперпараметра $\lambda$ (кросс-валидация)
5. Анализ остатков (проверка допущений)
6. Интерпретация результатов

---

# Кросс-валидация и подбор гиперпараметров

## Как выбрать lambda?

- **Цель**: Найти значение $\lambda$, дающее лучшую обобщающую способность
- **Метод**: k-Fold Cross-Validation
- **Процесс**: Разбиваем данные на k частей. k раз обучаем на (k-1) частях и валидируем на 1 части
- **Критерий**: Минимизация MSE на валидационных фолдах

---

# Сравнение методов регуляризации

## Ridge vs Lasso vs ElasticNet

| Критерий | Ridge (L2) | Lasso (L1) | ElasticNet |
|----------|------------|------------|------------|
| Отбор признаков | Нет | Да | Да |
| Коррелир. признаки | Сохраняет все | Оставляет один | Сохраняет группу |
| Интерпретация | Сложнее | Проще | Сложнее |
| Когда использовать? | Много полезных признаков | Много бесполезных признаков | Сильная корреляция + отбор |

---

# Резюме ключевых идей

## Главное, что нужно запомнить

- Минимизация MSE = ММП при нормальности ошибок
- Нарушение допущений ведет к некорректным выводам
- Регуляризация — мощный инструмент борьбы с переобучением
- Lasso отбирает признаки, Ridge — ужимает веса
- Всегда проверяйте остатки!

---

# Что дальше?

## Развитие темы

- **Обобщенная линейная модель (GLM)**: Для не-normal данных
- **Непараметрическая регрессия**: Ядерная регрессия, сплайны
- **Ансамбли на деревьях**: xgBoost, LightGBM
- **Нейронные сети** — универсальные аппроксиматоры

---

<!-- _class: title -->
![bg](https://images.unsplash.com/photo-1533750349088-cd871a92f312?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)

# **Спасибо за внимание!**

## Вопросы?

**Контакты**: Ваша почта / Telegram

---

# Источники и литература

## Литература

1. Hastie, Tibshirani, Friedman "The Elements of Statistical Learning"
2. James, Witten, Hastie, Tibshirani "An Introduction to Statistical Learning"
3. Andrew Ng "Machine Learning" (Coursera)
4. Sebastian Raschka "Python Machine Learning"
5. Документация sklearn.linear_model