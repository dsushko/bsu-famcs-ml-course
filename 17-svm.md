---
marp: true
theme: default
style: |
    section {
        font-family: Arial, sans-serif; font-size: 20px;
    }
    h1 {
        color: #023871ff;
        font-size: 2.5em;
    }
    h2 {
        color: #3e5268ff;
        border-bottom: 2px solid #0b1e32ff;
        padding-bottom: 0.3em;
    }
    strong {
        color: #11534f;
    }
    .columns {
        display: grid;
        grid-template-columns: repeat(2, minmax(0, 1fr));
        gap: 1rem;
    }
    .footer {
        position: absolute;
        bottom: 20px;
        font-size: 0.7em;
        color: gray;
    }
header: Support Vector Machine
footer: БГУ ФПМИ ФМИиС
---

<!-- _paginate: skip -->
<!-- _header: '' -->
<!-- _footer: '' -->
# **Support Vector Machine (SVM)**
## Машина опорных векторов. Принципы, сила и применение.
Ваше имя/название организации
`Дата: 26.08.2025`

---

<!-- _header: '' -->


1.  **Что такое SVM?** (Краткое введение)
2.  **Интуиция:** как работает SVM?
3.  **Математическая постановка** задачи (Оптимизационное уравнение)
4.  **Решение для нелинейных данных** (Soft Margin)
5.  **"Волшебный" трюк с ядром** (Kernel Trick)
6.  Популярные ядерные функции
7.  **Сильные стороны SVM:** Высокая размерность
8.  **Сравнение** с другими алгоритмами
9.  **Плюсы и минусы** SVM
10. **Итоги:** Когда использовать SVM?

---

## Что такое SVM?

*   **Контролируемый** алгоритм машинного обучения для **классификации** и регрессии.
*   **Ключевая идея:** Найти гиперплоскость, которая **максимизирует зазор** (margin) между классами.
*   **Цель:** Построение **устойчивой** модели, которая хорошо обобщается на новых данных.

![bg right:40% w:400](https://i.imgur.com/1N4ZtOE.png)
*Пример разделения классов гиперплоскостью*

---

## Интуиция: Лучшая граница принятия решений

**Вопрос:** "Какая разделяющая линия лучше?"
*   Линия **а**? **б**? **в**?

**Ответ:** Та, что находится на **максимальном расстоянии** от ближайших точек каждого класса.

![bg right:40% w:400](https://raw.githubusercontent.com/andreinechaev/svm_gif/master/svm_gif.gif)
*Поиск оптимальной гиперплоскости*

---

## Ключевые понятия

*   **Опорные векторы (Support Vectors):**
    Критически важные точки обучающей выборки, которые **определяют положение** гиперплоскости. Модель зависит только от них!

*   **Зазор (Margin):**
    "Буферная зона" вокруг гиперплоскости. SVM стремится **максимизировать** этот зазор.

![bg right:40% w:400](https://i.imgur.com/6gjDLr2.png)
*Гиперплоскость, зазор и опорные векторы*

---

## Математика. Часть 1: Формулировка задачи

*   **Цель:** Найти гиперплоскость `w·x - b = 0`.
*   **Условия для всех точек:**
    *   `w·x_i - b >= 1`, если `y_i = 1`
    *   `w·x_i - b <= -1`, если `y_i = -1`
    *   Объединенно: `y_i (w·x_i - b) >= 1`

*   **Ширина зазора (Margin):** `2 / ||w||`

**Вывод:** Чтобы максимизировать margin, нужно **минимизировать** `||w||`.

![bg right:40% w:350](https://i.imgur.com/9p2C7cN.png)
*Геометрический вывод ширины зазора*

---

## Математика. Часть 2: Оптимизационное уравнение

**Задача минимизации (Hard Margin):**

`Minimize:` $\frac{1}{2} ||\mathbf{w}||^2$

`Subject to:` $y_i (\mathbf{w} \cdot \mathbf{x}_i - b) \geq 1 \quad \text{для всех } i$

*   Ищем самую "простую" гиперплоскость (`min ||w||`), идеально разделяющую данные.
*   Решается методом **множителей Лагранжа**, что приводит к **двойственной задаче**.

![bg right:40% w:350](https://i.imgur.com/3mrSHnn.png)
*Вектор весов `w` перпендикулярен гиперплоскости*

---

## Реальность: Данные редко идеальны

**Проблема:** Жесткие условия (`>=1`) невыполнимы для нелинейно разделимых данных.

**Решение: Soft Margin SVM.**
Разрешаем алгоритму делать ошибки, но **штрафуем** за них.

**Вводим переменные "ошибок" ξ (кси):**
`y_i (w·x_i - b) >= 1 - ξ_i`, где `ξ_i >= 0`.

![bg right:40% w:400](https://i.imgur.com/2WrKKrJ.png)
*Жесткий (Hard) vs Мягкий (Soft) зазор*

---

## Оптимизационное уравнение для Soft Margin

**Задача минимизации (Soft Margin):**

`Minimize:` $\frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i} \xi_i$

`Subject to:` $y_i (\mathbf{w} \cdot \mathbf{x}_i - b) \geq 1 - \xi_i \quad \text{и} \quad \xi_i \geq 0 \quad \text{для всех } i$

**Параметр `C`:** Контролирует компромисс.
*   **`C` велик:** Жесткая классификация, риск **переобучения**.
*   **`C` мал:** "Мягкая" классификация, риск **недообучения**.

---

## Нелинейная классификация: Трюк с ядром

**Проблема:** Что если граница не линейна?

**Идея:** Отобразить данные в пространство **более высокой размерности**, где они становятся линейно разделимыми.

**Проблема проблемы:** Вычисления в новом пространстве **очень дороги**.

**Решение — Kernel Trick:** Работать в новом пространстве, **не вычисляя координаты точек в нем явно!**

![bg right:40% w:400](https://i.imgur.com/9Uj2uKz.png)
*Отображение из 2D в 3D пространство*

---

## Как работает Kernel Trick?

**Функция ядра:** `K(x_i, x_j) = φ(x_i) · φ(x_j)`

*   Вычисляет **скалярное произведение** образов векторов в высокоразмерном пространстве, работая **только с исходными векторами**.
*   **Аналогия:** Узнать, похожи ли два документа, не читая их полностью (не вычисляя их векторное представление).

![bg right:40% w:400](https://i.imgur.com/6kATl6e.png)
*Иллюстрация kernel trick*

---

## Популярные ядерные функции

*   **Линейное (Linear):** `K(x_i, x_j) = x_i · x_j`
*   **Полиномиальное (Polynomial):** `K(x_i, x_j) = (x_i · x_j + r)^d`
*   **Радиальная базисная функция (RBF) / Gaussian:**
    `K(x_i, x_j) = exp(-γ * ||x_i - x_j||²)`
    *   *Наиболее популярное. `γ` (gamma) — гиперпараметр, контролирующий гладкость границы.*
*   **Sigmoid:** `K(x_i, x_j) = tanh(α x_i · x_j + r)`

![bg right:40% w:400](https://i.imgur.com/4p43pQ5.png)
*Сравнение решающих границ разных ядер*

---

## Сила SVM №1: Эффективность в высокой размерности

*   **SVM исключительно хорошо работает, когда количество признаков (p) очень велико, даже > количества samples (n).**
    *   *Примеры: классификация текстов, биоинформатика (гены/образцы).*
*   **Причина:** Эффективность зависит не от исходной размерности, а от числа **опорных векторов**.

**Решающая функция:**
`f(x) = sign( Σ α_i y_i K(x_i, x) + b )`
*Зависит только от опорных векторов (α_i > 0).*

---

## Сила SVM №2: Устойчивость к переобучению

*   **Maximizing the margin** — это по своей сути форма **регуляризации**.
*   Алгоритм ищет не просто точное, но и **устойчивое** решение.
*   **Результат:** Хорошая обобщающая способность на новых данных.

![bg right:40% w:400](https://i.imgur.com/4W5qr6c.png)
*Сравнение SVM (устойчивая граница) и модели без max margin*

---

## Сравнение: SVM vs. Логистическая Регрессия

<div class="columns">
<div>

### **SVM**
*   **Подход:** Максимизация зазора (Margin)
*   **Граница:** Зависит только от **опорных векторов**
*   **Вероятности:** Нативно не дает
*   **Высокая размерность:** **Очень силен**

</div>
<div>

### **Логистическая Регрессия**
*   **Подход:** Максимизация правдоподобия
*   **Граница:** Зависит от **всех точек** данных
*   **Вероятности:** Дает вероятности **нативно**
*   **Высокая размерность:** Требует сильной регуляризации

</div>
</div>

---

## Сравнение: SVM vs. Деревья / Случайный Лес

<div class="columns">
<div>

### **SVM**
*   **Интерпретируемость:** "Черный ящик"
*   **Масштабирование:** **Требует**
*   **Нелинейность:** Через **ядра** (глобальная)
*   **Выбросы:** Устойчив (из-за `C`)

</div>
<div>

### **Деревья / Лес**
*   **Интерпретируемость:** **Очень высокая**
*   **Масштабирование:** Не требует
*   **Нелинейность:** Через **разделения** (локальная)
*   **Выбросы:** Чувствителен

</div>
</div>

---

## Сравнение: SVM vs. k-Nearest Neighbors (k-NN)

<div class="columns">
<div>

### **SVM**
*   **Скорость предсказания:** **Быстрое**
*   **Память:** **Экономичная** (только SV)
*   **Чувствительность к шуму:** Устойчива

</div>
<div>

### **k-NN**
*   **Скорость предсказания:** **Медленное**
*   **Память:** **Жадная** (все данные)
*   **Чувствительность к шуму:** Чувствителен

</div>
</div>

---

## Плюсы SVM (+)

1.  **Эффективен в пространствах высокой размерности.**
2.  **Память эффективна** — использует только опорные векторы.
3.  **Универсален:** Разные ядра для разных задач.
4.  **Устойчив к переобучению** (из-за max-margin).

---

## Минусы SVM (-)

1.  **Плохая интерпретируемость** модели ("черный ящик").
2.  **Медленный на больших датасетах** (>100k samples) — время обучения ~ O(n² - n³).
3.  **Требует подбора гиперпараметров** (`C`, `gamma`) и **масштабирования признаков**.
4.  **Не выдает вероятности** по умолчанию.

---

## Итоги: Когда использовать SVM?

*   ✅ Ваши данные **высокоразмерны** (тексты, гены, изображения).
*   ✅ Число признаков **превышает** число наблюдений.
*   ✅ Вам нужна **мощная нелинейная модель** с хорошим обобщением.
*   ✅ Размер датасета **не экстремально велик** (средний).
*   ❌ Вам **критически важна интерпретируемость**.
*   ❌ У вас **очень большой датасет** (миллионы строк).
*   ❌ Данные **сильно зашумлены** и перекрываются.

---

## Практические советы по применению

1.  **Всегда масштабируйте данные!** (StandardScaler, MinMaxScaler).
2.  **Начните с RBF-ядра** — лучший выбор по умолчанию.
3.  **Для подбора `C` и `gamma`** используйте GridSearchCV.
4.  Для линейных задач используйте `LinearSVC` — он часто быстрее.

![bg right:40% w:400](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_scale_candle_001.png)
*Влияние масштабирования на SVM*

---

## Примеры применения

*   **Распознавание лиц и изображений.**
*   **Классификация текстов и спам-фильтры.**
*   **Биоинформатика:** Классификация раковых тканей по экспрессии генов.
*   **Прогнозирование на финансовых рынках.**

![bg right:40% w:400](https://i.imgur.com/9U5sFpE.png)
*Использование SVM в компьютерном зрении*