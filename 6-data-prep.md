---
marp: true
title: "Подготовка данных: Фундамент для надежных моделей"
description: "Презентация о подготовке данных для машинного обучения"
paginate: true
theme: default
style: |
    section { font-family: Arial, sans-serif; font-size: 20px}
    h1 { color: #0056b3; }
    h2 { color: #007bff; }
    strong { color: #d63384; }
header: Подготовка данных
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: lead -->
# **Подготовка данных: Фундамент для надежных моделей**
## Как заставить линейную регрессию (и другие модели) работать на полную мощность

> "Garbage In – Garbage Out" (GIGO)

---

### Главный принцип машинного обучения
# Garbage In – Garbage Out (GIGO)

*   **Качество данных напрямую определяет** качество и надежность модели.
*   Можно иметь самый сложный алгоритм, но на **плохих данных он даст плохой результат**.
*   **Цель подготовки данных:** Сделать так, чтобы модель училась на закономерностях, а не на артефактах данных.


---

### Мотивация: Требования линейной регрессии
# Почему линейная регрессия особенно чувствительна?

*   **Чувствительность к масштабу:** Градиентный спуск сходится плохо, если признаки в разном масштабе.
*   **Чувствительность к выбросам:** Из-за квадратичной функции потерь (MSE).
*   **Требование к данным:** Предположение о нормальности распределения ошибок.
*   **Работа с категориями:** Модель требует числовых входов.
*   **Пропуски:** Не обрабатываются алгоритмами "из коробки".

---

# Что мы обсудим сегодня?

1.  **Масштабирование признаков**
2.  **Обработка категориальных признаков**
3.  **Заполнение пропущенных значений**
4.  **Удаление выбросов**
5.  **Преобразование целевой переменной**

---

### Проблема разных единиц измерения
# Зачем нужно масштабировать признаки?

*   **Признак А:** Зарплата (в рублях, диапазон 50 000 - 200 000)
*   **Признак Б:** Возраст (в годах, диапазон 18 - 80)
*   **Проблема:** Модель может посчитать, что зарплата "важнее" просто потому, что числа больше. Веса признаков становятся несопоставимы.
*   **Для градиентного спуска:** Без масштабирования траектория спуска к минимуму будет длинной и извилистой.

---

### Стандартизация (Z-score normalization)
# Методы масштабирования: StandardScaler

$$z = \frac{x - \mu}{\sigma}$$

*   Вычитает среднее значение и делит на стандартное отклонение.
*   Результат: Распределение с $\mu = 0$ и $\sigma = 1$.
*   **Когда использовать?** Почти всегда. Предполагает, что данные распределены нормально или примерно нормально.

---

### Нормализация (приведение к диапазону)
# Методы масштабирования: MinMaxScaler

$$X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$$

*   Сжимает все значения в диапазон `[0, 1]`.
*   **Когда использовать?** Когда важно сохранить нули в разреженных данных или когда распределение далеко от нормального.
*   **Минус:** Чувствителен к выбросам.

---

# Методы масштабирования: RobustScaler

*   **RobustScaler:** Использует медиану и межквартильный размах (IQR). $X_{scaled} = \frac{X - Q_1}{Q_3 - Q_1}$. На него почти не влияют выбросы.
*   **Normalizer:** Нормализует каждую строку (объект) до единичной нормы. Полезно для методов, основанных на близости (косинусное сходство).

## `StandardScaler` — универсальный выбор для линейных моделей.

---

### Важно: Не допустить "утечки данных" (Data Leakage)
# Практические шаги: Тренировка и применение

1.  **Разделить** данные на обучающую и тестовую выборку.
2.  **Настроить scaler** (вычислить $\mu$ и $\sigma$) **только на обучающих данных**.
3.  **Применить преобразование** к обучающим данным.
4.  **Применить преобразование** (с параметрами с шага 2) к тестовым данным.

> **Ошибка:** Вычислить параметры по всему датасету -> модель увидит "следы" тестовых данных.

---

### Числа vs Категории
# Типы категориальных признаков

*   **Номинальные (Nominal):** Категории без порядка (цвет: `"красный", "синий", "зеленый"`).
*   **Порядковые (Ordinal):** Категории с порядком (размер: `"S", "M", "L", "XL"`).
*   **Проблема:** Модель не понимает текст. Нам нужно перевести категории в числа, но так, чтобы не внести **ложный порядок**.

---

### Создание бинарных признаков-флагов
# One-Hot Encoding (OHE)

*   Для **номинальных** признаков.
*   Каждая категория становится новым бинарным признаком (0 или 1).
*   **Пример:** "Цвет" -> `is_red`, `is_blue`, `is_green`.
*   **Плюсы:** Полностью убирает ложный порядок.
*   **Минусы:** Резко увеличивает размерность данных (Проклятие размерности).

---

# Label Encoding и Ordinal Encoding

*   **Label Encoding:** Присваивает каждой категории число (0, 1, 2, ...). **Подходит только для деревьев!** Для линейных моделей создает ложный порядок.
*   **Ordinal Encoding:** Присваивает числа с учетом **настоящего порядка** (S=0, M=1, L=2, XL=3). Подходит для линейных моделей, если порядок настоящий.

---

### Кодирование средним значением целевой переменной
# Target Encoding (Mean Encoding)

*   **Идея:** Категория "Лондон" -> заменяется на среднюю цену квартир в Лондоне.
*   **Плюсы:** Вносит информацию о целевом признаке, часто хорошо работает.
*   **Опасность:** Сильная **переобучение**! Нужно очень аккуратно вычислять средние (например, использовать кросс-валидацию или сглаживание).
*   **Вывод:** Мощный, но опасный метод. Требует аккуратной валидации.

---

### Почему данные пропущены? Это важно!
# Типы пропусков: MCAR, MAR, MNAR

*   **MCAR (Missing Completely At Random):** Пропуск никак не связан с данными. Идеальный случай.
*   **MAR (Missing At Random):** Пропуск в одном признаке связан с другим признаком.
*   **MNAR (Missing Not At Random):** Пропуск связан с самим пропущенным значением. Самый сложный случай.

---

### Просто удалить пропуски
# Простые стратегии: Удаление

*   `df.dropna()` удаляет строки или столбцы с пропусками.
*   **Плюсы:** Простота.
*   **Минусы:** Потеря информации. Может внести смещение (bias), если пропуски не MCAR.
*   **Когда использовать?** Когда пропусков очень мало (<5%) и они скорее всего MCAR.

---

### Imputation — Заполнение пропусков
# Простые стратегии: Заполнение константой и статистикой

*   **Заполнение константой:** -1, 0, `UNKNOWN`. Прозрачно указывает на пропуск, но может исказить данные.
*   **Заполнение статистикой:** Для чисел — **медиана** (устойчива к выбросам) или среднее. Для категорий — **мода** (самое частое значение).
*   **Минус:** Искусственно занижает дисперсию признака. Не учитывает взаимосвязи между признаками.

---

### Импутация с учетом других признаков
# Продвинутые стратегии: KNN и Iterative Imputer

*   **KNN Imputer:** Для объекта с пропуском ищутся k ближайших соседей, и пропуск заполняется средним/медианой значения у этих соседей.
*   **Iterative Imputer (MICE):** Каждый признак с пропусками моделируется как функция от других признаков.
*   **Плюсы:** Учитывает взаимосвязи в данных.
*   **Минусы:** Вычислительно дорогой. Риск переобучения.

---

### Аномальное наблюдение
# Что такое выброс и почему он опасен?

*   **Выброс:** Значение, которое сильно отличается от остальных наблюдений.
*   **Опасность для линейной регрессии:** Квадратичная функция потерь (MSE) придает выбросам огромный вес. Линия регрессии сильно "перекосится" в сторону выброса.

![bg right:40% w:400](https://mathworld.wolfram.com/images/eps-svg/OutlierScatterplot_1000.svg)

---

# Методы обнаружения: IQR (Interquartile Range)

$$IQR = Q_3 - Q_1$$
Нижняя граница: 
$$ Q_1 - 1.5 \cdot IQR$$
Верхняя граница: 
$$ Q_3 + 1.5 \cdot IQR$$

*   Значения за этими границами считаются "мягкими" выбросами.
*   Можно использовать множитель 3 для "экстремальных" выбросов.

---

# Методы обнаружения: Z-score

$$z = \frac{x - \mu}{\sigma}$$

*   Значения с |Z-score| > 3 (реже 2.5 или 3.5) считаются выбросами.
*   **Важно:** Сам метод чувствителен к выбросам (т.к. использует $\mu$ и $\sigma$).

---

# Методы обнаружения: На глазок

Учитывая природу данных и их бизнес-контекст, самостоятельно по гистограмме найти такие *квантили*, которые по данной переменной означают выброс. 

Часто намного эффективнее всех вышеуказанных методов, так как в реальности данные и их распределения бывают совершенно разными.

![bg right:40% w:450](https://i.sstatic.net/JD0cW.gif)

---

# Методы обработки: Удаление и Capping

*   **Удаление:** Просто удалить строки с выбросами. **Риск:** потеря информации.
*   **Capping (Winsorization):** "Подрезать" выбросы. Все значения выше верхней границы приравниваются к верхней границе, ниже нижней — к нижней. Более щадящий метод.
*   **Важно:** Выбросы — это не всегда ошибка. Иногда это самая важная информация! Нужно понимать природу данных.

---

### Вернемся к вероятностным предположениям
# Зачем преобразовывать целевую переменную?

*   Линейная регрессия предполагает, что **ошибки распределены нормально**.
*   Если целевая переменная имеет сильно скошенное распределение (цены, зарплаты), то и ошибки будут иметь не нормальное распределение.
*   **Решение:** Преобразовать целевую переменную так, чтобы ее распределение стало более нормальным.

---

### Самое популярное преобразование
# Логарифмическое преобразование

$$y_{new} = \log(y)$$

*   **Когда использовать?** Когда данные имеют правостороннюю асимметрию (длинный хвост справа).
*   **Эффект:** "Сжимает" большие значения, растягивает маленькие. Делает распределение более симметричным.
*   **Интерпретация:** Модель предсказывает не сам $y$, а $\log(y)$. При обратном преобразовании ($e^{\hat{y}}$) мы получаем оценку **медианы**, а не среднего значения.

---

### Автоматический подбор преобразования
# Другие преобразования (Box-Cox, Yeo-Johnson)

*   **Преобразование Бокса-Кокса (Box-Cox):** 
$$y(\lambda) = \frac{y^\lambda - 1}{\lambda}$$ 
для y > 0. Автоматически подбирает параметр $\lambda$.
*   **Преобразование Йео-Джонсона (Yeo-Johnson):** Обобщение Box-Cox, которое работает и с отрицательными значениями.
*   **Важно:** Все параметры преобразования ($\lambda$) должны подбираться на **обучающей** выборке!

---

### Сработало ли преобразование?
# Валидация: Проверяем результат

*   **Визуально:** Гистограмма и Q-Q plot до и после преобразования.
*   **Статистические тесты:** Тест Шапиро-Уилка на нормальность.
*   **Главный критерий:** Улучшились ли метрики на валидационной выборке и стали ли остатки модели более нормальными?

---

### Порядок имеет значение!
# Общий пайплайн подготовки данных

**Важная последовательность!**
1.  **Разделение** на train/test.
2.  Обработка **выбросов** (в тренировочных данных).
3.  Заполнение **пропусков** (параметры — с тренировочных данных).
4.  Кодирование **категорий** (параметры — с тренировочных данных).
5.  **Масштабирование** (параметры — с тренировочных данных).
6.  Преобразование **целевой переменной** (параметры — с тренировочных данных).
7.  **Применение** всех обученных трансформеров к тестовой выборке.

---

### Автоматизация для избежания Data Leakage
# Инструменты: Scikit-learn pipelines

*   `sklearn.pipeline.Pipeline` позволяет объединить все этапы предобработки и модель в один объект.
*   **Плюсы:** Исключает человеческую ошибку, код становится чище, легко делать кросс-валидацию.
*   **Пример:**
    `pipe = make_pipeline(SimpleImputer(), StandardScaler(), LinearRegression())`

---

### Резюме: Стоило ли оно того?
# Плюсы и минусы сложной подготовки данных

**Плюсы:**
✅ Значительное улучшение качества и надежности моделей.
✅ Корректные статистические выводы и интерпретация.
✅ Стабильность и скорость работы алгоритмов оптимизации.

**Минусы:**
❌ Трудоемкий и долгий процесс.
❌ Риск внести свои собственные ошибки и смещения.
❌ Сложность воспроизведения и развертывания.

---

# Выводы

*   Подготовка данных — **критически важный** этап.
*   Все параметры трансформаций должны считаться на **тренировочной** выборке.
*   Выбор методов зависит от **природы данных** и **модели**.
*   Всегда смотрите на **распределения** и **остатки** до и после преобразований.