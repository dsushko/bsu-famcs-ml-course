---
marp: true
title: "Логистическая регрессия: От вероятности к классификации"
description: "Презентация о логистической регрессии для задачи классификации"
paginate: true
theme: default
style: |
    section { font-family: Arial, sans-serif; font-size: 20px}
    h1 { color: #0056b3; }
    h2 { color: #007bff; }
    strong { color: #d63384; }
header: Классификация. Логистическая регрессия
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: lead -->
# **Логистическая регрессия: От вероятности к классификации**
### Линейный классификатор с вероятностной интерпретацией


---

### Предсказание категории или метки
# Задача классификации

*   **Дано:** Признаковые описания объектов: $X = (x_1^{(i)}, x_2^{(i)}, ..., x_m^{(i)})$, $i=1..n$.
*   **Найти:** Дискретный целевой вектор $y = (y^{(1)}, y^{(2)}, ..., y^{(n)})$, где $y^{(i)} \in \{0, 1, ..., K\}$.
*   **Бинарная классификация:** $K=1$ ($y \in \{0, 1\}$).
*   **Цель:** Найти функцию $f(X)$, которая предсказывает метку класса.

---

### Непригодность линейной регрессии для классификации
# Почему не линейная регрессия?

*   Линейная регрессия предсказывает **непрерывные значения**, выходящие за пределы `[0, 1]`.
*   **Чувствительна к выбросам.**
*   Предположение о нормальности ошибок **нарушается**.
*   Нужна функция, **"сжимающая"** прогноз в интервал `[0, 1]` для интерпретации как вероятности.

---

### Ошибка классификации и Accuracy
# Метрики качества классификации (Часть 1)

**Confusion Matrix (Матрица ошибок):**
*   **True Positive (TP), True Negative (TN)**
*   **False Positive (FP)** (Type I Error), **False Negative (FN)** (Type II Error)

**Accuracy (Доля верных ответов):**
$$\frac{TP + TN}{TP + TN + FP + FN}$$

**Недостаток Accuracy:** Не информативна на несбалансированных выборках.

![bg right:30% w:600](https://www.blog.trainindata.com/wp-content/uploads/2024/09/confusion-matrix-1.png)

---

### Precision, Recall и F-мера
# Метрики качества классификации (Часть 2)

Представим себе задачу классификации - определить, любит ли человек *python*?

*   **Precision (Точность):** $\frac{TP}{TP + FP}$. Качество положительных прогнозов.
Отвечает на вопрос:
*Если мы предсказали человека как питониста, то с какой вероятностью так и есть?*

*   **Recall (Полнота):** $\frac{TP}{TP + FN}$. Способность найти все positive объекты.
Отвечает на вопрос:
*Сколько питонистов от общего числа мы обнаружили?*

*   **F-мера (F1-score):** Гармоническое среднее Precision и Recall.
$$F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$

**Когда использовать?** При несбалансированных данных или разной стоимости ошибок FP и FN.

![bg right:25% w:350](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)

---

### ROC-кривая и AUC-ROC
# Метрики качества классификации (Часть 3)

*   **ROC-кривая:** Строится для модели, выдающей вероятность. Показывает зависимость **TPR (Recall)** от **FPR** ($\frac{FP}{TN+FP}$) при изменении порога классификации.
*   **AUC-ROC (Area Under Curve):** Площадь под ROC-кривой. Показывает способность модели **ранжировать** объекты.
*   **Интерпретация:** `0.5` — случайный классификатор, `1.0` — идеальный. Однако сложно сказать, как интерпретировать значения посередине, в зависимости от датасета смысл чисел будет разный. 
ROC-AUC как метрика больше подходит как индикатор изменения общего качества модели при настройке её параметров - ROC-AUC выше прошлого значения, скорее всего, будет значить более хорошую модель. 
Т.е это скорее Data Science метрика, чем бизнесовая

![bg right:40% w:95%](https://media.geeksforgeeks.org/wp-content/uploads/20230410164437/AUC-ROC-Curve.webp)

---

### От линейной комбинации к вероятности
# Идея логистической регрессии

1.  **Шаг 1:** Вычисляем линейную комбинацию (логит): $z = \langle w, x \rangle + b$.
2.  **Шаг 2:** "Пропускаем" логит через **сигмоидную (логистическую)** функцию.
3.  **Сигмоида:** $\sigma(z) = \frac{1}{1 + e^{-z}}$.
4.  **Результат:** $\hat{p} = P(y=1 | x) = \sigma(\langle w, x \rangle + b)$.
![bg right:40% w:500](https://ml-explained.com/articles/logistic-regression-explained/logistic_regression_decision_boundary.png)
---

### Свойства сигмоидной функции
# Сигмоидная функция (График)

*   $\sigma(z) \in (0, 1)$ для любого $z$.
*   $\sigma(0) = 0.5$.
*   Монотонно возрастает.
*   Имеет красивые производные: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$.

![bg right:40% w:95%](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)

---

### Почему не MSE? Функция кросс-энтропии
# Функция потерь (Log Loss)

*   **MSE** для вероятностей невыпуклая, имеет много локальных минимумов.
*   **Log Loss (Binary Cross-Entropy):**
$$L(w) = - \frac{1}{n} \sum_{i=1}^{n} [y_i \cdot \log(\hat{p_i}) + (1 - y_i) \cdot \log(1 - \hat{p_i})]$$
*   **Интуиция:** Сильно штрафует за уверенные, но неправильные прогнозы.
*   **Свойства:** Выпуклая по параметрам $w$, удобна для оптимизации.

---

### Вычисление градиента
# Градиентный спуск для логистической регрессии

*   **Цель:** Минимизировать $L(w)$.
*   **Градиент:**
$$\nabla_w L(w) = \frac{1}{n} X^T (\hat{p} - y)$$
*   **Обновление весов:**
$$w_{new} = w_{old} - \eta \cdot \nabla_w L(w_{old})$$
*   **Замечание:** Формула градиента похожа на градиент для линейной регрессии ($X^T (\hat{y} - y)$), но $\hat{p}$ вычисляется через сигмоиду.

---

### Модель порождения данных
# Вероятностная постановка задачи

*   Целевая переменная $y$ подчиняется **распределению Бернулли**.
*   $y_i | x_i \sim \text{Bernoulli}(p_i)$, где $p_i = P(y_i=1 | x_i)$.
*   Задача модели: Предсказать параметр $p_i$ распределения Бернулли по признакам $x_i$.

---

### Функция правдоподобия для Бернулли
# Вывод функции правдоподобия

*   **Правдоподобие:** Вероятность наблюдать данные при заданных параметрах $w$.
$$L(w) = P(Data | w) = \prod_{i=1}^{n} P(y_i | x_i, w) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}$$

---

### Максимизация правдоподобия
# Принцип максимального правдоподобия (ММП)

*   Ищем $w$, которые **максимизируют** $L(w)$.
*   Практичнее работать с **логарифмом** правдоподобия.
$$\log L(w) = \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]$$

---

### Максимизация правдоподобия = Минимизация перекрестной энтропии
# Связь ММП и Log Loss

*   $\log L(w) = - n \cdot \text{Log Loss}$ (с точностью до знака и константы).
*   **Ключевой вывод:** Максимизация $\log L(w)$ **эквивалентна** минимизации Log Loss.
*   Минимизация Log Loss является оптимальной с точки зрения **ММП** для задачи бинарной классификации.

---

### Необходимость контроля сложности
# Переобучение в логистической регрессии

*   При большом количестве признаков модель может подстроиться под **шум** в данных.
*   Веса могут стать слишком **большими** по модулю.
*   **Решение:** Добавить **штраф** за большие веса к функции потерь.

---

### Аналогично линейной регрессии
# Виды регуляризации (L1 и L2)

*   **L2-регуляризация (Ridge):** Штрафует за сумму квадратов весов.
$L_{L2}(w) = \text{Log Loss}(w) + \lambda \sum_{j=1}^{m} w_j^2$. Веса **сжимаются**.
*   **L1-регуляризация (Lasso):** Штрафует за сумму модулей весов.
$L_{L1}(w) = \text{Log Loss}(w) + \lambda \sum_{j=1}^{m} |w_j|$. Веса **обнуляются**. Полезно для отбора признаков.
*   **ElasticNet:** Комбинация L1 и L2.

---

### Байесовский взгляд: Априорные распределения
# Вероятностная интерпретация регуляризации (MAP)

*   **Без регуляризации:** ММП -> $w_{ММП} = \arg\max_w P(Data | w)$.
*   **С регуляризацией:** MAP -> $w_{MAP} = \arg\max_w P(w | Data) = \arg\max_w P(Data | w) \cdot P(w)$.
*   **Априор $P(w)$:**
    *   **L2:** Эквивалентно априору $w_j \sim \mathcal{N}(0, \frac{1}{\lambda})$.
    *   **L1:** Эквивалентно априору $w_j \sim \text{Laplace}(0, \frac{1}{\lambda})$.

---

### Более двух классов
# Задача многоклассовой классификации (Multiclass)

*   $y \in \{0, 1, 2, ..., K\}$, где $K > 2$.
*   **Примеры:**
    *   Классификация рукописных цифр (10 классов)
    *   Классификация типов цветков (3 класса)

---

### Простой и эффективный подход
# Стратегия "один против всех" (One-vs-Rest)

*   Для **каждого класса** $k$ обучается отдельный бинарный классификатор.
*   Классификатор $k$ отвечает на вопрос: "Объект принадлежит классу $k$ или нет?".
*   **Проблема:** Несбалансированность. "Зона неопределенности", если несколько классификаторов выдали высокую вероятность.

---

### Много классификаторов, но на подмножествах
# Стратегия "один против одного" (One-vs-One)

*   Для **каждой пары классов** $(i, j)$ обучается бинарный классификатор.
*   Всего обучается $\frac{K(K-1)}{2}$ классификаторов.
*   **Голосование:** Новый объект прогоняется через все классификаторы. Класс, набравший большинство голосов, выбирается как ответ.
*   **Плюс:** Каждый классификатор обучается на сбалансированной подвыборке.
*   **Минус:** Большое количество моделей при росте $K$.

---

### Прямое обобщение на многоклассовый случай
# Мультиномиальная логистическая регрессия (Softmax)

*   Обобщение сигмоиды: **Softmax function**.
$$\hat{p}_k = P(y=k | x) = \frac{e^{\langle w_k, x \rangle}}{\sum_{j=1}^{K} e^{\langle w_j, x \rangle}} \quad \text{для } k = 1, ..., K$$
*   **Интерпретация:** Softmax преобразует $K$ логитов в вероятности, так что сумма по всем классам равна 1.

---

### Перекрестная энтропия для множества классов
# Функция потерь и градиент для Softmax

*   **Categorical Cross-Entropy:**
$$L(\{w_k\}) = - \frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{I}[y_i = k] \cdot \log(\hat{p}_{i,k})$$
*где $\mathbb{I}[y_i = k]$ — индикаторная функция (1, если истина, 0 иначе).*
*   **Градиент:**
$$\nabla_{w_k} L = \frac{1}{n} X^T (\hat{p}_k - \mathbb{I}_{y=k})$$
*   **Аналогия** с градиентом в бинарном случае ($X^T(\hat{p} - y)$) становится очевидной.

---

### Несколько меток одновременно
# Задача многометковой классификации (Multilabel)

*   Каждому объекту может быть приписано **несколько меток (классов)** одновременно.
*   $y_i$ — это уже не число, а **бинарный вектор** длины $K$.
*   **Пример:** Классификация текстов. Документ может относиться к темам "политика", "экономика" и "коррупция" одновременно.

---

### Преобразование в несколько бинарных задач
# Подходы к решению (Problem Transformation)

*   **Наивный подход (Binary Relevance):** Для каждой метки $k$ обучается **независимый** бинарный классификатор. Игнорирует связи между метками.
*   **Более сложные:** Classifier Chains (выход одного классификатора является признаком для другого), Label Powerset.

---

### Изменение самих алгоритмов
# Подходы к решению (Algorithm Adaptation)

*   **Логистическая регрессия для Multilabel:**
    *   По сути, то же самое, что и **Binary Relevance**.
    *   Обучаем $K$ **независимых** бинарных логистических регрессий, по одной на каждую метку.
    *   Каждая модель выдает вероятность принадлежности объекта своей метке.
*   **Важно:** Порог отсечения для каждой метки можно подбирать независимо.

---

### Сильные стороны
# Плюсы логистической регрессии

*   🟢 **Высокая интерпретируемость:** Веса показывают влияние признака на вероятность.
*   🟢 **Эффективность:** Быстрое обучение и предсказание.
*   🟢 **Выход — вероятность:** Можно оценить "уверенность" модели.
*   🟢 **Хорошая baseline-модель** для задач классификации.
*   🟢 **Регуляризация:** Легко борется с переобучением.

---

### Ограничения
# Минусы логистической регрессии

*   🔴 **Линейная граница решения:** Плохо работает с данными, где классы нельзя разделить линейно.
*   🔴 **Предположение о линейной зависимости:** Логит линейно зависит от признаков.
*   🔴 **Чувствительность** к коррелированным признакам и выбросам.

---

### Логистическая регрессия vs Деревья vs SVM
# Сравнение с другими методами

*   **Деревья решений:** Нелинейны, не требуют масштабирования, менее интерпретируемы при росте глубины.
*   **SVM (линейное ядро):** Также строит линейную границу, но оптимизирует зазор (margin), а не вероятность. Менее интерпретируема.
*   **Вывод:** Логистическая регрессия — лучший выбор, когда нужна **вероятность** и **интерпретация**.

---

# Выводы

*   Логистическая регрессия предсказывает **вероятность** класса.
*   Основана на **ММП** для распределения Бернулли.
*   Оптимизирует **логарифмическую** функцию потерь (кросс-энтропию).
*   Легко обобщается на **многоклассовый** и **многометковый** случай.
*   **Регуляризация** критически важна для борьбы с переобучением.