---
marp: true
theme: default
style: |
    section {
        font-family: Arial, sans-serif; font-size: 20px
    }
paginate: true
header: Методы кластеризации в машинном обучении
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: title -->
# **Методы кластеризации: Искусство нахождения структуры в данных**
## K-Means, EM-алгоритм, иерархическая кластеризация, DBSCAN и оценка качества

![bg right w:650](https://media.geeksforgeeks.org/wp-content/uploads/20200703030848/Screenshotfrom20200703025113.png)

---

### **Введение в кластеризацию**

<!-- _class: -->
# **Что такое кластеризация?**
### Обучение без учителя (Unsupervised Learning)

*   **Цель:** Разбить множество объектов на **группы (кластеры)** так, чтобы:
    *   Объекты внутри кластера были **похожи**.
    *   Объекты из разных кластеров были **непохожи**.
*   **Ключевое отличие:** Исходные метки классов **отсутствуют**.
*   **Задачи:** Сегментация клиентов, поиск аномалий, сжатие данных.

![bg right:40% w:650](https://i.sstatic.net/6GNLF.jpg)

---

# **Основные типы кластеризации**
### Разные подходы к группировке

*   **Центроидная (K-Means):** Кластер описывается его центром.
*   **Вероятностная (GMM):** Кластер описывается распределением вероятностей.
*   **Связная / Иерархическая:** Кластеры образуют древовидную структуру.
*   **Плотностная (DBSCAN):** Кластер — это плотное облако точек.

---

![bg w:600](https://substackcdn.com/image/fetch/$s_!eRX9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2880da35-e13c-4522-b60f-22b62b3fb97a_3669x3606.jpeg)

---

### **K-Means и EM-алгоритм**

<!-- _class: -->
# **Алгоритм K-Means**
### Идея и постановка задачи

*   **Цель:** Минимизировать внутрикластерную дисперсию.
*   **Функция потерь:** $\sum_{i=1}^{n} \min_{j=1..K} ||x_i - \mu_j||^2$
*   **Параметр:** `K` — количество кластеров (задается пользователем).

---

# **Алгоритм K-Means (Шаги)**
### Итеративный процесс уточнения

1.  **Инициализация:** Выбрать `K` начальных центроидов.
2.  **Шаг присваивания (E-step):** Для каждой точки найти ближайший центроид.
3.  **Шаг обновления (M-step):** Пересчитать центроиды кластеров.
4.  **Повторять** шаги 2-3 до сходимости.

![bg right:40% w:500](https://miro.medium.com/1*rwYaxuY-jeiVXH0fyqC_oA.gif)

---

# **Проблемы и особенности K-Means**
### Ограничения метода

*   **Чувствительность к начальной инициализации.** Решение: **K-Means++**.
*   **Предполагает сферические кластеры** одинакового размера.
*   **Чувствительность к выбросам.**
*   **Требует задания числа `K`.**

---
# K-Means++

K-Means++ — это не отдельный алгоритм, а умная процедура инициализации (выбора начальных центроидов) для стандартного K-Means. Его цель — выбрать центры так, чтобы они были разнесены друг от друга и хорошо представляли структуру данных, что приводит к более точным и быстрым результатам.

* Первый центроид: Выбирается случайно и равномерно из всех точек данных. Это самый простой шаг.
* Последующие центроиды: Для каждого из оставшихся k-1 центроидов:

* Для каждой точки данных вычисляется квадрат расстояния (D(x)²) до ближайшего из уже выбранных центроидов.

* Чем точка дальше от всех существующих центров, тем больше будет это значение.
Все эти квадраты расстояний суммируются.

* Следующий центроид выбирается случайно, но с вероятностью, пропорциональной вычисленному квадрату расстояния для каждой точки.

* Формула вероятности для точки x: P(x) = D(x)² / Σ D(i)²

* Запуск стандартного K-Means

---

# **GMM**
### Вероятностный подход к кластеризации

*   **Идея:** Данные порождены не сферами, а **смесью нескольких гауссовых распределений**.
*   **Кластер** описывается параметрами: **среднее ($\mu$)** и **ковариационная матрица ($\Sigma$)**.
*   **Ковариация** позволяет моделировать эллиптические, вытянутые кластеры.

![bg right:50% w:400](https://substackcdn.com/image/fetch/$s_!PNq2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8480e23f-beac-4944-bd9d-487a31c86dab_523x340.png)
![bg vertical right:50% w:500](https://keeeto.github.io/ebook-data-analysis/_images/lecture-2-clustering-kmeans-GMM_32_0.png)


---


# **Сравнение K-Means и GMM**
### Жесткая vs мягкая кластеризация

| Критерий | K-Means | GMM |
| :--- | :--- | :--- |
| **Тип присваивания** | Жесткое (0 или 1) | Мягкое (вероятность) |
| **Форма кластеров** | Сферическая | Эллиптическая (гибкая) |
| **Скорость** | Быстро | Медленно |
| **Устойчивость** | Чувствителен к выбросам | Более устойчив |

**Вывод:** GMM — это обобщение K-Means на случай мягких кластеров сложной формы.

---

# **EM-алгоритм (Expectation-Maximization)**
### Общая схема для обучения моделей со скрытыми переменными (такие как K-Means, GMM, и другие)

*   **Скрытые переменные:** Мы не знаем, к какому распределению принадлежит точка.
*   **E-шаг (Expectation):** Оцениваем вероятности принадлежности точек к кластерам. Каждый алгоритм задаёт конкретную формулу этого шага по-своему.
*   **M-шаг (Maximization):** Пересчитываем параметры распределений ($\mu$, $\Sigma$), чтобы они были максимально "правдоподобными" к присвоенным точкам. Пересчёт также зависит от конкретого метода
*   **Шаги повторяются до сходимости**

![bg right:40% w:500](https://www.researchgate.net/publication/329773081/figure/fig19/AS:718943555686411@1548420909950/llustrative-example-of-the-EM-algorithm-for-estimation-of-parameters-of-a-GMM-with-2.ppm)

---

### **Иерархическая кластеризация**

<!-- _class: -->
# **Идея иерархической кластеризации**
### Построение дерева кластеров (дендрограммы)

*   **Цель:** Построить **иерархию** вложенных разбиений.
*   **Два подхода:**
    *   **Агломеративная (восходящая):** Объединяем два самых близких кластера.
    *   **Дивизивная (нисходящая):** Разделяем кластеры.
*   **Результат:** **Дендрограмма**.

![bg right:40% w:500](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Iris_dendrogram.png/250px-Iris_dendrogram.png)

---

# **Методы расчета расстояния между кластерами**
### Как измерить близость двух групп точек?

*   **Single Link (ближайшего соседа):** Расстояние между ближайшими точками.
*   **Complete Link (дальнего соседа):** Расстояние между самыми дальними точками.
*   **Average Link:** Среднее расстояние.
*   **Ward's Method:** Минимизирует **общую внутрикластерную дисперсию**.


---

# **Метод Уорда (Ward)**
### Наиболее популярный метод

*   **Цель:** Минимизировать дисперсию внутри кластеров.
*   **Расстояние:** $D(A, B) = \frac{||\vec{\mu_A} - \vec{\mu_B}||^2}{\frac{1}{|A|} + \frac{1}{|B|}}$
*   **Плюсы:** Склонен к созданию кластеров одинакового размера.
*   **Минусы:** Плохо работает с кластерами сложной формы.

---

### **DBSCAN (Density-Based Clustering)**

<!-- _class: -->
# **Идея DBSCAN**
### Кластеризация на основе плотности

*   **Ключевая идея:** Кластер — это **область с высокой плотностью** точек.
*   **Решает проблемы K-Means:**
    *   Находит кластеры **любой формы**.
    *   **Устойчив к выбросам** (помечает их как шум).
    *   **Не требует задания числа кластеров `K`.**

![bg right:50% w:600](https://rust-ml.github.io/book/assets/clustering_comparison.png)

---

# **Основные понятия DBSCAN**
### Терминология

*   **Параметры:** `eps` (радиус окрестности), `min_samples`.
*   **Точка ядра (Core Point):** В её окрестности есть не менее `min_samples` точек.
*   **Граничная точка (Border Point):** Попала в окрестность точки ядра, но сама не ядро.
*   **Шумовая точка (Noise Point):** Не ядро и не граничная.

---

# **Алгоритм DBSCAN**
### Пошаговое описание

1.  Пометить все точки как непосещенные.
2.  Выбрать непосещенную точку.
3.  Если она **точка ядра** -> начать формирование нового кластера.
4.  Добавить все **достижимые по плотности** точки.
5.  Если не ядро -> пометить как шум.
6.  Повторять, пока все точки не посещены.

---

# **Преимущества и недостатки DBSCAN**
### Плюсы и минусы

**Плюсы:**
*   🟢 Не требует задания числа кластеров.
*   🟢 Находит кластеры произвольной формы.
*   🟢 Устойчив к выбросам.

**Минусы:**
*   🔴 Плохо работает с кластерами разной плотности.
*   🔴 Чувствителен к подбору параметров.
*   🔴 Плохо работает в высокоразмерных пространствах.

---

# Какой метод кластеризации подойдёт лучше в данной задаче?

* Найти группы перефразировок вопросов\фраз с одинаковым смыслом

* Произвести сегментацию всей базы абонентов на некоторое количество кластеров по признакам

* Найти кластеры в данных по результатам исследований зависимости одних показателей от других

---

# **Метрики оценки качества кластеризации**

*   **Внешние метрики (External):** Истинные метки **известны**. Для валидации, сравнения разных кластеризаций.
*   **Внутренние метрики (Internal):** Истинные метки **неизвестны**. Для подбора параметров.

---
# Внешние метрики: ARI, AMI

разобраться самостоятельно!

---

# K-Means, Elbow method

* **Суть проблемы**: K-Means требует, чтобы мы задали число кластеров k заранее. Но как выбрать это число, если мы ничего не знаем о структуре данных? Метод локтя — это простой и интуитивный эвристический подход для решения этой проблемы.

* Что такое **Inertia**?
Inertia — это сумма квадратов расстояний от каждой точки данных до центроида того кластера, которому она принадлежит. Эта величина высчитывается автоматически для любой итоговой кластеризации K-means.

* Основная идея: с увеличением числа кластеров `k` Inertia будет улучшаться, но после определенного момента это улучшение будет незначительным. Задача — найти точку, где выигрыш в качестве резко падает.

![bg right:40% w:600](https://miro.medium.com/v2/resize:fit:670/0*aY163H0kOrBO46S-.png)

---

# **Внутренние метрики (Silhouette Score)**
### Насколько объекту комфортно в своем кластере?

*   Для точки $i$:
    *   $a(i)$ — среднее расстояние до других точек в **своем** кластере.
    *   $b(i)$ — среднее расстояние до точек в **ближайшем** кластере.
*   **Silhouette:** $s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$
*   **Общий Score:** Среднее по всем точкам. **Диапазон: [-1, 1]**.
* Эта метрика часто используется в жизни и даёт более очевидное представление о оптимальном числе кластеров нежели метод локтя.

![bg right:50% w:600](https://media.geeksforgeeks.org/wp-content/uploads/20250703152720891959/elbow_sil.png)

---

# **Сравнение метрик**
### Какую метрику выбрать?

| Метрика | Тип | Когда использовать |
| :--- | :--- | :--- |
| **ARI, AMI** | Внешняя | Если есть ground truth |
| **Elbow mthod** | Внутренняя | Для подбора `k` в К-Means (в реальности почти не используется) |
| **Silhouette** | Внутренняя | Для подбора `k`, оценки компактности |

**Важно:** Ни одна внутренняя метрика не является идеальной.

---

# **Как выбрать метод?**
### Практические рекомендации

*   **Быстрый baseline** -> **K-Means**.
*   Нужно назначить вероятности или кластеры эллиптической формы -> **GMM**.
*   Наглядное дерево, неизвестно `K` -> **Иерархическая** (малые данные).
*   Кластеры сложной формы или выбросы -> **DBSCAN**.
*   **Всегда** визуализируйте результат.

---

# **Итого**

*   Кластеризация — инструмент **обучения без учителя**.
*   **K-Means** — быстрый метод для сферических кластеров.
*   **GMM** — обобщение K-Means на мягкие кластеры сложной формы.
*   **Иерархическая** кластеризация строит дендрограмму.
*   **DBSCAN** находит кластеры произвольной формы и распознаёт выбросы.
*   Для оценки используйте **внешние (ARI/AMI)** и **внутренние (Silhouette)** метрики.