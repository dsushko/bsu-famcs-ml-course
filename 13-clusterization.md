---
marp: true
theme: default
style: |
    section {
        font-family: Arial, sans-serif; font-size: 20px
    }
paginate: true
header: Методы кластеризации в машинном обучении
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: title -->
# **Методы кластеризации: Искусство нахождения структуры в данных**
## K-Means, EM-алгоритм, иерархическая кластеризация, DBSCAN и оценка качества
![bg right](https://source.unsplash.com/800x600/?cluster,data,butterfly,pattern)

---

### **Введение в кластеризацию**

<!-- _class: -->
# **Что такое кластеризация?**
### Обучение без учителя (Unsupervised Learning)

*   **Цель:** Разбить множество объектов на **группы (кластеры)** так, чтобы:
    *   Объекты внутри кластера были **похожи**.
    *   Объекты из разных кластеров были **непохожи**.
*   **Ключевое отличие:** Исходные метки классов **отсутствуют**.
*   **Задачи:** Сегментация клиентов, поиск аномалий, сжатие данных.

![bg right:40%](https://source.unsplash.com/400x300/?unsupervised,learning,pattern)

---

# **Основные типы кластеризации**
### Разные подходы к группировке

*   **Центроидная (K-Means):** Кластер описывается его центром.
*   **Вероятностная (GMM):** Кластер описывается распределением вероятностей.
*   **Связная / Иерархическая:** Кластеры образуют древовидную структуру.
*   **Плотностная (DBSCAN):** Кластер — это плотное облако точек.

![bg right:40%](https://source.unsplash.com/400x300/?groups,types,categories)

---

### **K-Means и EM-алгоритм**

<!-- _class: -->
# **Алгоритм K-Means**
### Идея и постановка задачи

*   **Цель:** Минимизировать внутрикластерную дисперсию.
*   **Функция потерь:** $\sum_{i=1}^{n} \min_{j=1..K} ||x_i - \mu_j||^2$
*   **Параметр:** `K` — количество кластеров (задается пользователем).

![bg right:40%](https://source.unsplash.com/400x300/?centroid,center,mean)

---

# **Алгоритм K-Means (Шаги)**
### Итеративный процесс уточнения

1.  **Инициализация:** Выбрать `K` начальных центроидов.
2.  **Шаг присваивания (E-step):** Для каждой точки найти ближайший центроид.
3.  **Шаг обновления (M-step):** Пересчитать центроиды кластеров.
4.  **Повторять** шаги 2-3 до сходимости.

![bg right:40%](https://source.unsplash.com/400x300/?iteration,process,steps)

---

# **Визуализация работы K-Means**
### Наглядный пример

![w:900](https://source.unsplash.com/900x400/?kmeans,algorithm,clustering-steps)

---

# **Проблемы и особенности K-Means**
### Ограничения метода

*   **Чувствительность к начальной инициализации.** Решение: **K-Means++**.
*   **Предполагает сферические кластеры** одинакового размера.
*   **Чувствительность к выбросам.**
*   **Требует задания числа `K`.**

![bg right:40%](https://source.unsplash.com/400x300/?problem,challenge,limitation)

---

# **Гауссовские смеси распределений (GMM)**
### Вероятностный подход к кластеризации

*   **Идея:** Данные порождены **смесью нескольких гауссовых распределений**.
*   **Кластер** описывается параметрами: **среднее ($\mu$)** и **ковариационная матрица ($\Sigma$)**.
*   **Ковариация** позволяет моделировать эллиптические, вытянутые кластеры.

![bg right:40%](https://source.unsplash.com/400x300/?gaussian,distribution,curve)

---

# **EM-алгоритм (Expectation-Maximization)**
### Общая схема для обучения моделей с скрытыми переменными

*   **Скрытые переменные:** Мы не знаем, к какому распределению принадлежит точка.
*   **E-шаг (Expectation):** Оцениваем вероятности принадлежности точек к кластерам.
*   **M-шаг (Maximization):** Пересчитываем параметры распределений ($\mu$, $\Sigma$).
*   **Аналогия с K-Means:** E-шаг ~ шаг присваивания, M-шаг ~ шаг обновления.

![bg right:40%](https://source.unsplash.com/400x300/?expectation,maximization,statistics)

---

# **Сравнение K-Means и GMM**
### Жесткая vs мягкая кластеризация

| Критерий | K-Means | GMM |
| :--- | :--- | :--- |
| **Тип присваивания** | Жесткое (0 или 1) | Мягкое (вероятность) |
| **Форма кластеров** | Сферическая | Эллиптическая (гибкая) |
| **Скорость** | Быстрее | Медленнее |
| **Устойчивость** | Чувствителен к выбросам | Более устойчив |

**Вывод:** GMM — это обобщение K-Means на случай мягких кластеров сложной формы.

---

### **Иерархическая кластеризация**

<!-- _class: -->
# **Идея иерархической кластеризации**
### Построение дерева кластеров (дендрограммы)

*   **Цель:** Построить **иерархию** вложенных разбиений.
*   **Два подхода:**
    *   **Агломеративная (восходящая):** Объединяем два самых близких кластера.
    *   **Дивизивная (нисходящая):** Разделяем кластеры.
*   **Результат:** **Дендрограмма**.

![bg right:40%](https://source.unsplash.com/400x300/?hierarchy,tree,structure)

---

# **Методы расчета расстояния между кластерами**
### Как измерить близость двух групп точек?

*   **Single Link (ближайшего соседа):** Расстояние между ближайшими точками.
*   **Complete Link (дальнего соседа):** Расстояние между самыми дальними точками.
*   **Average Link:** Среднее расстояние.
*   **Ward's Method:** Минимизирует **общую внутрикластерную дисперсию**.

![bg right:40%](https://source.unsplash.com/400x300/?distance,measure,geometry)

---

# **Метод Уорда (Ward)**
### Наиболее популярный метод

*   **Цель:** Минимизировать дисперсию внутри кластеров.
*   **Расстояние:** $D(A, B) = \frac{||\vec{\mu_A} - \vec{\mu_B}||^2}{\frac{1}{|A|} + \frac{1}{|B|}}$
*   **Плюсы:** Склонен к созданию кластеров одинакового размера.
*   **Минусы:** Плохо работает с кластерами сложной формы.

---

# **Дендрограмма и выбор числа кластеров**
### Как интерпретировать результат?

*   **Вертикальная ось:** Расстояние, на котором объединяются кластеры.
*   **Выбор `K`:** "Разрезаем" дендрограмму на нужной высоте.
*   **Плюсы:** Не требует задания `K` заранее, наглядная визуализация.
*   **Минусы:** Вычислительная сложность $O(n^3)$.

![w:700](https://source.unsplash.com/700x400/?dendrogram,tree,chart)

---

### **DBSCAN (Density-Based Clustering)**

<!-- _class: -->
# **Идея DBSCAN**
### Кластеризация на основе плотности

*   **Ключевая идея:** Кластер — это **область с высокой плотностью** точек.
*   **Решает проблемы K-Means:**
    *   Находит кластеры **любой формы**.
    *   **Устойчив к выбросам** (помечает их как шум).
    *   **Не требует задания числа кластеров `K`.**

![bg right:40%](https://source.unsplash.com/400x300/?density,cloud,points)

---

# **Основные понятия DBSCAN**
### Терминология

*   **Параметры:** `eps` (радиус окрестности), `min_samples`.
*   **Точка ядра (Core Point):** В её окрестности есть не менее `min_samples` точек.
*   **Граничная точка (Border Point):** Попала в окрестность точки ядра, но сама не ядро.
*   **Шумовая точка (Noise Point):** Не ядро и не граничная.

![bg right:40%](https://source.unsplash.com/400x300/?core,noise,terminology)

---

# **Алгоритм DBSCAN**
### Пошаговое описание

1.  Пометить все точки как непосещенные.
2.  Выбрать непосещенную точку.
3.  Если она **точка ядра** -> начать формирование нового кластера.
4.  Добавить все **достижимые по плотности** точки.
5.  Если не ядро -> пометить как шум.
6.  Повторять, пока все точки не посещены.

---

# **Преимущества и недостатки DBSCAN**
### Плюсы и минусы

**Плюсы:**
*   🟢 Не требует задания числа кластеров.
*   🟢 Находит кластеры произвольной формы.
*   🟢 Устойчив к выбросам.

**Минусы:**
*   🔴 Плохо работает с кластерами разной плотности.
*   🔴 Чувствителен к подбору параметров.
*   🔴 Плохо работает в высокоразмерных пространствах.

---

### **Метрики оценки качества кластеризации**

<!-- _class: -->
# **Зачем оценивать качество кластеризации?**
### Сравнение алгоритмов и подбор параметров

*   **Внешние метрики (External):** Истинные метки **известны**. Для валидации.
*   **Внутренние метрики (Internal):** Истинные метки **неизвестны**. Для подбора параметров.
*   **Относительные метрики (Relative):** Сравнение разных кластеризаций.

![bg right:40%](https://source.unsplash.com/400x300/?quality,metric,assessment)

---

# **Внешние метрики (Adjusted Rand Index - ARI)**
### Попарное сравнение объектов

*   **Идея:** Сравниваем два разбиения: наше и истинное.
*   **Adjusted Rand Index:** Исправляет случайное угадывание.
*   **Диапазон: [-1, 1]**. 1 — полное совпадение.

---

# **Внешние метрики (Adjusted Mutual Information - AMI)**
### Информационно-теоретический подход

*   **Взаимная информация (MI):** Измеряет, насколько много информации о одном разбиении содержится в другом.
*   **Adjusted Mutual Information:** Исправляет случайность.
*   **Свойства:** Значение 1 при полном совпадении, ~0 при случайном разбиении.

---

# **Внутренние метрики (Silhouette Score)**
### Насколько объекту комфортно в своем кластере?

*   Для точки $i$:
    *   $a(i)$ — среднее расстояние до других точек в **своем** кластере.
    *   $b(i)$ — среднее расстояние до точек в **ближайшем** кластере.
*   **Silhouette:** $s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$
*   **Общий Score:** Среднее по всем точкам. **Диапазон: [-1, 1]**.

![bg right:40%](https://source.unsplash.com/400x300/?silhouette,score,comfort)

---

# **Внутренние метрики (Calinski-Harabasz Index)**
### Отношение межкластерной дисперсии к внутрикластерной

*   **Формула:** $CH = \frac{\text{trace}(B_k) / (k-1)}{\text{trace}(W_k) / (n-k)}$
*   **Интуиция:** Высокое значение = кластеры **плотные** и **хорошо разделены**.
*   **Применение:** Подбор `K` в K-Means (выбирают `K` с максимальным CH).

---

# **Сравнение метрик**
### Какую метрику выбрать?

| Метрика | Тип | Когда использовать |
| :--- | :--- | :--- |
| **ARI, AMI** | Внешняя | Если есть ground truth |
| **Silhouette** | Внутренняя | Для подбора `K`, оценки компактности |
| **Calinski-Harabasz** | Внутренняя | Для подбора `K` в K-Means |

**Важно:** Ни одна внутренняя метрика не является идеальной.

---

### **Сравнение и применение**

<!-- _class: -->
# **Сравнение методов кластеризации**
### Итоговая таблица

| Метод | Форма кластеров | Задает `K`? | Уст. к шуму | Скорость |
| :--- | :--- | :--- | :--- | :--- |
| **K-Means** | Сферическая | Да | Нет | Быстрая |
| **GMM** | Эллиптическая | Да | Средняя | Средняя |
| **Иерархическая** | Разная | Нет* | Нет | Медленная |
| **DBSCAN** | Произвольная | Нет | **Да** | Средняя |

---

# **Как выбрать метод?**
### Практические рекомендации

*   **Быстрый baseline** -> **K-Means**.
*   Вероятностное assignment или эллипсы -> **GMM**.
*   Наглядное дерево, неизвестно `K` -> **Иерархическая** (малые данные).
*   Кластеры сложной формы или выбросы -> **DBSCAN**.
*   **Всегда** визуализируйте результат.

---

# **Проблема определения числа кластеров (`K`)**
### Elbow Method и не только

*   **Elbow Method:** График зависимости inertia от `K`. Выбираем `K` на "изгибе".
*   **Анализ дендрограммы:** Ищем самые длинные вертикальные линии.
*   **Внутренние метрики:** Max Silhouette Score или Calinski-Harabasz Index.
*   **Важно:** Это эвристики. Выбор `K` зависит от предметной области.

![w:700](https://source.unsplash.com/700x400/?elbow,method,graph,chart)

---

# **Ключевые выводы**
### Takeaways

*   Кластеризация — мощный инструмент **обучения без учителя**.
*   **K-Means** — быстрый метод для сферических кластеров.
*   **GMM** — обобщение K-Means на мягкие кластеры сложной формы.
*   **Иерархическая** кластеризация строит дендрограмму.
*   **DBSCAN** находит кластеры произвольной формы и детектирует выбросы.
*   Для оценки используйте **внешние (ARI/AMI)** и **внутренние (Silhouette)** метрики.