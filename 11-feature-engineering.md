---
marp: true
theme: default
style: |
    section {
        font-family: Arial, sans-serif; font-size: 20px
    }
    h1 {
        color: #2C3E50;
    }
    h2 {
        color: #3498DB;
    }
    strong {
        color: #E74C3C;
    }
header: Feature engineering
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: lead -->
# **Обработка признаков: От сырых данных к качественным моделям**
## Отбор и конструирование признаков — что действительно важно для ML

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*_6M4nk6sEkj7_ESM4nqirA.png)

---

### **Главный секрет успеха в ML**
# Качество данных > Сложность алгоритма

*   **Хорошие признаки** часто важнее выбора сложного алгоритма.
*   **Цель:** Представить алгоритму данные в максимально понятной и информативной форме.
*   **Две основные задачи:**
    1.  **Отбор признаков (Feature Selection):** Выбрать лучшее подмножество из существующих.
    2.  **Конструирование признаков (Feature Engineering):** Создать новые, более информативные признаки.
*   **Результат:** Более простые, быстрые и интерпретируемые модели с лучшим качеством.

![bg right:30% w:300](https://www.researchgate.net/publication/349721420/figure/fig1/AS:997402575429637@1614624819942/Feature-engineering-and-feature-selection-process.png)

---

### **Что будет, если этого не делать?**
# Проблемы, которые решает обработка признаков

*   **Проклятие размерности:** Слишком много признаков -> модель запоминает шум, а не закономерности (переобучение).
*   **Мультиколлинеарность:** Взаимосвязь признаков мешает работе линейных моделей.
*   **Избыточность:** Признаки не несут новой информации, но замедляют обучение.
*   **Низкое качество прогноза:** Модель не может выучить сложные зависимости из "бедных" признаков.

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*RfqlC4dkskLQX1KpO5tnaA.png)

---

### **Отбор признаков (Feature Selection)**
## **Зачем отбирать признаки?**
# Меньше — значит лучше

*   **Ускорение обучения и предсказания.** Меньше данных -> быстрее вычисления.
*   **Уменьшение переобучения.** Меньше шума -> лучше обобщающая способность.
*   **Улучшение интерпретируемости** модели. Легче понять, что на что влияет.
*   **Упрощение визуализации** данных.

![bg right:40% w:400](https://scikit-learn.org/stable/_images/sphx_glr_plot_feature_selection_001.png)

---

## **Методы отбора признаков (Filter Methods)**
# Отбор на основе статистических критериев

*   **Идея:** Признаки оцениваются по их **информативности** относительно целевой переменной *до* обучения модели.
*   **Методы:**
    *   **Для числовых признаков:** Корреляция Пирсона, дисперсионный анализ (ANOVA).
    *   **Для категориальных:** $\chi^2$ (хи-квадрат), взаимная информация (Mutual Information).
*   **Плюсы:** Быстро, не зависит от модели.
*   **Минусы:** Игнорирует взаимодействие признаков.

![bg right:40% w:400](https://www.researchgate.net/publication/336925481/figure/fig3/AS:819588297416704@1572603280530/Feature-selection-using-Pearson-Correlation-Method.png)

---

## **Методы отбора признаков (Wrapper Methods)**
# Отбор с помощью самой ML-модели

*   **Идея:** "Завернуть" процесс отбора в алгоритм обучения. Использовать качество модели как критерий отбора.
*   **Методы:**
    *   **Recursive Feature Elimination (RFE):** Построить модель -> убрать самый слабый признак -> повторить.
    *   **Forward/Backward Selection:** Жадный поиск. Добавлять/удалять по одному признаку, выбирая лучший/худший.
*   **Плюсы:** Учитывает взаимодействие признаков, часто дает лучшее качество.
*   **Минусы:** Медленно (нужно много раз обучать модель).

![bg right:40% w:400](https://media.geeksforgeeks.org/wp-content/uploads/20240527151812/Recursive-Feature-Elimination-RFE.webp)

---

## **Методы отбора признаков (Embedded Methods)**
# Отбор "на лету" в процессе обучения

*   **Идея:** Сам алгоритм обучения встроенным образом проводит отбор признаков.
*   **Методы:**
    *   **L1-регуляризация (Lasso):** "Обнуляет" веса у неважных признаков.
    *   **Важность признаков в деревьях (Random Forest, XGBoost):** Ранжирование признаков по вкладу в прогноз.
*   **Плюсы:** Быстро (не требует отдельного шага), эффективно.
*   **Минусы:** Привязано к конкретной модели.

![bg right:40% w:400](https://www.researchgate.net/publication/358787850/figure/fig1/AS:1115383188918272@1643963654865/Feature-selection-using-LASSO-regression-A-Bar-plot-showing-the-features-selected-by.png)

---

### **Конструирование признаков (Feature Engineering)**
## **Зачем создавать новые признаки?**
# Дать модели больше возможностей для обучения

*   Помочь линейным моделям уловить **нелинейные зависимости**.
*   Сжать информацию и уменьшить размерность.
*   Представить данные в более **понятном для алгоритма** виде.
*   Это творческий процесс, требующий **знания предметной области**.

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*6s0IPU1Hc2w2M6lKFrT-A.png)

---

## **Конструирование в табличных данных (Числовые признаки)**
# Преобразование существующих полей

*   **Биннинг (Binning):** `Возраст` -> `Возрастная_группа` (0-18, 19-30, 31+). Помогает линейным моделям.
*   **Полиномиальные признаки:** Создать `x²`, `x³`, `x₁ * x₂`. Позволяет уловить нелинейность.
*   **Логарифмирование:** `log(Зарплата)`. Для работы с правосторонним skewness.
*   **Агрегаты:** `Средний_чек_пользователя`, `Количество_операций_за_день`.

![bg right:40% w:400](https://www.researchgate.net/publication/362001748/figure/fig1/AS:11431281085358024@1658568224209/Feature-engineering-techniques-applied-on-the-dataset.ppm)

---

## **Конструирование в табличных данных (Категориальные признаки)**
# Работа с категориями — это не только OHE

*   **Frequency Encoding:** Замена категории на частоту ее встречаемости в данных.
*   **Target Encoding (Mean Encoding):** Замена категории на среднее значение целевой переменной по этой категории. **Осторожно: риск переобучения!**
*   **Создание новых категорий:** Объединение редких категорий в `Other`.

![bg right:40% w:400](https://docs.rapidminer.com/latest/studio/operators/images/encoding_nominal_to_numerical.svg)

---

## **Конструирование в табличных данных (Время и Дата)**
# Временные ряды — золотая жила для feature engineering

*   **Разбиение даты:** `Год`, `Месяц`, `День`, `День_недели`, `Квартал`, `Это_выходной?`.
*   **Время с момента события:** `Дней_с_последней_покупки`.
*   **Извлечение цикличности:** `sin(день_года)`, `cos(день_года)` для моделирования сезонности.

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*_6M4nk6sEkj7_ESM4nqirA.png)

---

## **Конструирование в нетабличных данных (Текст)**
# От мешка слов к смыслу

*   **Bag of Words / TF-IDF:** Классические подходы, представляют текст как вектор частот слов.
*   **N-граммы:** Учитывают сочетания слов (например, "очень крутой" vs "очень" + "крутой").
*   **Извлечение сущностей (NER):** Поиск в тексте имен, названий компаний, мест.
*   **Эмбеддинги (Word2Vec, BERT):** Современные методы, представляющие слова/предложения в виде плотных векторов, сохраняющих семантику.

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*5SRO3kq2Qc7j_0c6cN2QZg.png)

---

## **Конструирование в нетабличных данных (Изображения)**
# Признаки — это не только пиксели

*   **Признаки "ручной работы" (Hand-crafted):**
    *   **HOG (Histogram of Oriented Gradients):** Для обнаружения объектов.
    *   **SIFT, SURF:** Для поиска и сопоставления ключевых точек.
*   **Использование предобученных CNN:** Взятие признаков из промежуточных слоев сверточной нейросети (например, VGG, ResNet) как готового векторного представления изображения.

![bg right:40% w:400](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/HOG.png/440px-HOG.png)

---

## **Автоматическое конструирование признаков**
# Может ли машина делать это сама?

*   **Библиотеки (например, FeatureTools):** Автоматически генерируют тысячи признаков с помощью агрегаций ( sum, mean, count, etc.) по связанным таблицам.
*   **Генетическое программирование:** Эволюционный поиск лучших формул для создания новых признаков.
*   **Плюсы:** Экономит время, может найти неочевидные зависимости.
*   **Минусы:** Может создать много мусора, требует последующего жесткого отбора.

![bg right:40% w:400](https://featuretools.com/wp-content/uploads/2017/12/2-Deep-Feature-Synthesis-1-1024x518.png)

---

<!-- _class: lead -->
# **Выводы**
## Ключевые takeaways

*   **Feature Selection** и **Feature Engineering** — критически важные этапы пайплайна.
*   **Отбор** делает модели быстрее, стабильнее и понятнее.
*   **Конструирование** — это творчество, которое сильно повышает качество моделей, особенно при знании предметной области.
*   **Лучшие практики:** Все преобразования нужно учить на тренировочной выборке, чтобы избежать data leakage.
*   **Помните:** Качество ваших данных определяет потолок качества вашей модели.

![bg opacity](https://miro.medium.com/v2/resize:fit:1400/1*_6M4nk6sEkj7_ESM4nqirA.png)