---
marp: true
theme: default
style: |
    section {
        font-family: Arial, sans-serif; font-size: 20px
    }
    h1 {
        color: #2C3E50;
    }
    h2 {
        color: #3498DB;
    }
    strong {
        color: #E74C3C;
    }
header: Feature engineering
footer: БГУ ФПМИ ФМИиС
---

<!-- _class: lead -->
# **Обработка признаков: От сырых данных к качественным моделям**
## Отбор и конструирование признаков — что действительно важно для ML

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*_6M4nk6sEkj7_ESM4nqirA.png)

---

### **Главный секрет успеха в ML**
# Качество данных > Сложность алгоритма

*   **Хорошие признаки** часто важнее выбора сложного алгоритма.
*   **Цель:** Представить алгоритму данные в максимально понятной и информативной форме.
*   **Две основные задачи:**
    1.  **Отбор признаков (Feature Selection):** Выбрать лучшее подмножество из существующих.
    2.  **Конструирование признаков (Feature Engineering):** Создать новые, более информативные признаки.
*   **Результат:** Более простые, быстрые и интерпретируемые модели с лучшим качеством.

![bg right:30% w:300](https://www.researchgate.net/publication/349721420/figure/fig1/AS:997402575429637@1614624819942/Feature-engineering-and-feature-selection-process.png)

---

### **Что будет, если этого не делать?**
# Проблемы, которые решает обработка признаков

*   **Проклятие размерности:** Слишком много признаков -> модель запоминает шум, а не закономерности (переобучение).
*   **Мультиколлинеарность:** Взаимосвязь признаков мешает работе линейных моделей.
*   **Избыточность:** Признаки не несут новой информации, но замедляют обучение.
*   **Низкое качество прогноза:** Модель не может выучить сложные зависимости из "бедных" признаков.

![bg right:40% w:400](https://miro.medium.com/v2/resize:fit:1400/1*RfqlC4dkskLQX1KpO5tnaA.png)

---

### **Отбор признаков (Feature Selection)**
## **Зачем отбирать признаки?**
# Меньше — значит лучше

*   **Ускорение обучения и предсказания.** Меньше данных -> быстрее вычисления.
*   **Уменьшение переобучения.** Меньше шума -> лучше обобщающая способность.
*   **Улучшение интерпретируемости** модели. Легче понять, что на что влияет.
*   **Упрощение визуализации** данных.

![bg right:40% w:400](https://scikit-learn.org/stable/_images/sphx_glr_plot_feature_selection_001.png)

---

## **Методы отбора признаков (Filter Methods)**
# Отбор на основе статистических критериев

*   **Идея:** Признаки оцениваются по их **информативности** относительно целевой переменной *до* обучения модели.
*   **Методы:**
    *   **Для числовых признаков:** Корреляция Пирсона, дисперсионный анализ (ANOVA).
    *   **Для категориальных:** $\chi^2$ (хи-квадрат), взаимная информация (Mutual Information).
*   **Плюсы:** Быстро, не зависит от модели.
*   **Минусы:** 
    - Игнорирует взаимодействие признаков.
    - Можно ошибочно выбрать признаки с проблемой correlation/causation.

![bg right:40% w:500](https://media.licdn.com/dms/image/v2/D5612AQGunviFk-bacw/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1667686635779?e=2147483647&v=beta&t=Q1w0VhGzO54BOh3HJc-0qn_3Gpc1Nh_KlT7s_Y0UG_E)

---

## **Методы отбора признаков (Wrapper Methods)**
# Отбор с помощью самой ML-модели

*   **Идея:** "Завернуть" процесс отбора в алгоритм обучения. Использовать качество модели как критерий отбора.
*   **Методы:**
    *   **Recursive Feature Elimination (RFE):** Построить модель -> убрать самый слабый признак -> повторить.
    *   **Forward/Backward Selection:** Жадный поиск. Добавлять/удалять по одному признаку, выбирая лучший/худший.
*   **Плюсы:** Учитывает взаимодействие признаков, часто дает лучшее качество.
*   **Минусы:** Медленно (нужно много раз обучать модель).

---

## **Методы отбора признаков (Embedded Methods)**
# Отбор "на лету" в процессе обучения

*   **Идея:** Сам алгоритм обучения встроенным образом проводит отбор признаков.
*   **Методы:**
    *   **L1-регуляризация (Lasso):** "Обнуляет" веса у неважных признаков.
    *   **Важность признаков в деревьях (Random Forest, XGBoost):** Ранжирование признаков по вкладу в прогноз.
*   **Плюсы:** Быстро (не требует отдельного шага), эффективно.
*   **Минусы:** Привязано к конкретной модели.

![bg right:40% w:500](https://towardsdatascience.com/wp-content/uploads/2022/05/1ZeINTX82W7vwqLMHHWEaTQ.jpeg)

---

### **Конструирование признаков (Feature Engineering)**
## **Зачем создавать новые признаки?**
# Дать модели больше возможностей для обучения

*   Помочь линейным моделям уловить **нелинейные зависимости**.
*   Сжать информацию и уменьшить размерность.
*   Представить данные в более **понятном для алгоритма** виде.
*   Это творческий процесс, требующий **знания предметной области**.

---

## **Конструирование в табличных данных (Числовые признаки)**
# Преобразование существующих полей

*   **Биннинг (Binning):** `Возраст` -> `Возрастная_группа` (0-18, 19-30, 31+). Помогает линейным моделям.
*   **Полиномиальные признаки:** Создать `x²`, `x³`, `x₁ * x₂`. Позволяет уловить нелинейность.
*   **Логарифмирование:** `log(Зарплата)`. Для работы с правосторонним skewness.
*   **Агрегаты:** `Средний_чек_пользователя`, `Количество_операций_за_день`.

---

## **Конструирование в табличных данных (Категориальные признаки)**
# Работа с категориями — это не только OHE

*   **Frequency Encoding:** Замена категории на частоту ее встречаемости в данных.
*   **Target Encoding (Mean Encoding):** Замена категории на среднее значение целевой переменной по этой категории. **Осторожно: риск переобучения!**
*   **Создание новых категорий:** Объединение редких категорий в `Other`.

![bg vertical right:40% w:500](https://miro.medium.com/v2/resize:fit:1400/0*mzU8quf7t-FHJETT.png)
![bg vertical right:40% w:500](https://habrastorage.org/getpro/habr/upload_files/d85/93e/527/d8593e527edced50d1d22c8e32a05b85.png)

---

## **Конструирование в табличных данных (Время и Дата)**
# Временные ряды — золотая жила для feature engineering

*   **Разбиение даты:** `Год`, `Месяц`, `День`, `День_недели`, `Квартал`, `Это_выходной?`.
*   **Время с момента события:** `Дней_с_последней_покупки`.
*   **Извлечение цикличности:** `sin(день_года)`, `cos(день_года)` для моделирования сезонности.

![bg right:45% w:550](https://freemanbrain.space/images/Pasted-image-20231024082626.png)

---

## **Автоматическое конструирование признаков**
# Может ли машина делать это сама?

*   **Библиотеки (например, FeatureTools):** Автоматически генерируют тысячи признаков с помощью агрегаций ( sum, mean, count, etc.) по связанным таблицам.
*   **Генетическое программирование:** Эволюционный поиск лучших формул для создания новых признаков.
*   **Плюсы:** Экономит время, может найти неочевидные зависимости.
*   **Минусы:** Может создать много мусора, требует последующего жесткого отбора.

---

<!-- _class: lead -->
# **Итого**

*   **Feature Selection** и **Feature Engineering** — критически важные этапы пайплайна.
*   **Отбор** делает модели быстрее, стабильнее и понятнее.
*   **Конструирование** — это творчество, которое сильно повышает качество моделей, особенно при знании предметной области.
*   Все преобразования нужно учить на тренировочной выборке, чтобы избежать data leakage.
*   Качество ваших данных определяет потолок качества вашей модели.

![bg opacity](https://miro.medium.com/v2/resize:fit:1400/1*_6M4nk6sEkj7_ESM4nqirA.png)