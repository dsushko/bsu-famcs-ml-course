---
marp: true
theme: default
style: |
    section {
        font-family: 'Arial', sans-serif;
        font-size: 20px;
    }
paginate: true
header: Валидация и тестирование моделей машинного обучения
footer: БГУ ФПМИ ФМИиС
---

# Не учись на экзаменационных билетах: Валидация и тестирование моделей

## Как оценить, будет ли ваша модель работать в реальном мире

---

# Обобщающая способность (Generalization)

- Конечная цель любой модели — делать точные предсказания на новых, ранее не виденных данных
- Хорошая производительность на обучающих данных — это необходимое, но недостаточное условие
- Ключевой вопрос: Как мы можем оценить эту способность к обобщению до развертывания модели?

---

# Что такое переобучение?

- Модель становится слишком сложной и подстраивается под шум и случайные особенности обучающих данных
- Результат: Отличные результаты на тренировочных данных, но плохие на новых
- Аналогия: Студент, который зазубрил билеты, но не понял тему

![bg right:40% width:400](https://miro.medium.com/v2/resize:fit:1400/1*_7OPgojau8hkiPUiHoGK_w.png)

---

# Имитация "новых данных"

- Мы искусственно прячем часть данных от модели на этапе обучения
- Эта hidden часть данных используется для симуляции реального мира
- Основные множества:
  - **Обучающая выборка**: Данные для обучения модели
  - **Валидационная выборка**: Данные для настройки гиперпараметров
  - **Тестовая выборка**: Данные для финальной оценки

---

# Самый простой подход: Hold-Out

- Разделяем весь датасет на две части случайным образом
- Типичное соотношение: 70%/30% или 80%/20%
- **Процесс:**
  1. Обучить модель на Train
  2. Оценить качество на Test
  3. Больше не прикасаться к Test до самого конца!

**Плюсы:** Простота и скорость  
**Минусы:** Оценка зависит от случайного разбиения

---

# Где настраивать гиперпараметры?

- **Hyperparameters**: Параметры, не обучаемые моделью
- **Validation Set**: Отдельная часть данных для подбора гиперпараметров
- Типичное соотношение: 60%/20%/20% или 70%/15%/15%
- **Важно:** Тестовый набор все еще полностью изолирован

---

# Случайное vs Стратифицированное разбиение

- **Случайное разбиение**: Простое случайное перемешивание и разделение
- **Стратифицированное разбиение**: Сохраняет пропорции классов
- **Зачем?** Критически важно для несбалансированных datasets

---

# Неэффективное использование данных

- Мы жертвуем большой частью данных для валидации/теста
- Оценка может быть неустойчивой (зависит от одного случайного разбиения)
- **Решение:** Кросс-валидация — более надежный и экономный способ

---

# Золотой стандарт оценки: K-Fold Cross-Validation

1. Данные случайно shuffl'ятся и разбиваются на k одинаковых частей
2. Модель обучается k раз
3. Итоговая оценка — среднее по всем k валидационным фолдам

![bg right:40% width:400](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)

---

# Практические аспекты K-Fold CV

- **Выбор K**: Чаще всего k=5 или k=10
- **Stratified K-Fold**: Для задач классификации
- **Для чего используется?**
  - Надежная оценка обобщающей способности
  - Сравнение моделей и выбор лучшей
  - Подбор гиперпараметров

---

# Правильный пайплайн

1. Разделить данные на Train и Test (80/20)
2. На Train части проводить всю настройку модели с K-Fold CV
3. Выбрать лучшую модель
4. Один раз провести финальную оценку на Test set

**Важно:** Кросс-валидация происходит внутри тренировочного множества

---

# Случай крайне малых данных: Leave-One-Out (LOOCV)

- k = n (количество объектов в выборке)
- Каждый раз модель обучается на всех данных, кроме одного объекта
- **Плюс:** Максимально использует данные для обучения
- **Минус:** Крайне вычислительно дорого

---

# Для временных рядов (Time-Aware CV)

- **Важно:** Запрещено случайное разбиение!
- **Принцип:** Будущее не должно влиять на прошлое
- **Методы:**
  - Rolling Window: Фиксированный размер обучающего окна
  - Expanding Window: Обучающее окно увеличивается

---

# Что оптимизирует алгоритм? Функция потерь

- **Функция потерь (Loss)**: Функция, которую алгоритм минимизирует
- Непрерывная, дифференцируемая (для вычисления градиента)
- **Примеры:** MSE, Log Loss, Huber Loss
- Может не иметь прямой интерпретации для бизнеса

---

# Что действительно интересует бизнес? Метрика качества

- **Метрика (Metric)**: Понятная человеку мера качества модели
- Не обязательно дифференцируема
- **Примеры:** Accuracy, Precision, Recall, F1-Score, ROC-AUC, R²
- **Ключевой момент:** Метрика и функция потерь могут не совпадать

---

# Loss vs. Metric

| Критерий | Функция потерь (Loss) | Метрика (Metric) |
|----------|----------------------|------------------|
| Цель | Научить алгоритм | Оценить модель |
| Требования | Должна быть дифференцируема | Должна быть интерпретируема |
| Пример | Log Loss | F1-Score |

**Как выбирать метрику?** По бизнес-требованиям

---

# Практический кейс несовпадения Loss и Metric

- **Задача:** Обнаружение мошеннических транзакций
- **Функция потерь:** Log Loss (для устойчивого обучения)
- **Метрика:** F1-Score или Precision@Recall=0.9
- **Действия:** Выбираем модель, максимизируя F1-Score

---

# Слишком сложная модель: Суть переобучения

- Модель имеет слишком большую вместимость (capacity)
- Она запоминает шум вместо общей закономерности
- Результат: Низкая ошибка на training set, высокая на test set

---

# Недообучение vs Оптимум vs Переобучение

![bg width:700](https://miro.medium.com/v2/resize:fit:1400/1*JZ5lsqoJxx2xII0s8qKJbw.png)

---

# Математическая интерпретация: Bias-Variance Decomposition

Ожидаемая ошибка модели распадается на три части:

- **Смещение (Bias)**: Ошибка из-за слишком простых предположений
- **Разброс (Variance)**: Чувствительность к изменениям в данных
- **Неустранимая ошибка**: Шум в данных

**Bias-Variance Tradeoff:** Нельзя минимизировать и bias, и variance одновременно

---

# Методы борьбы с переобучением

- Сбор большего количества данных
- Упрощение модели
- Регуляризация (L1-Lasso, L2-Ridge)
- Ансамбли (Bagging - Random Forest)
- Ранняя остановка (Early Stopping)

---

# Диагностика переобучения: Learning Curves

- График зависимости ошибки от размера обучающей выборки
- **Переобучение:** Большой разрыв между train и validation error
- **Недообучение:** Обе ошибки высоки и сходятся

![bg right:40% width:350](https://scikit-learn.org/stable/_images/plot_learning_curve_001.png)

---

# Скрытая угроза: Data Leakage (Утечка данных)

- Ситуация, когда в признаки попадает информация, недоступная в реальном прогнозе
- Результат: Слишком оптимистичные оценки, провал в продакшене
- **Пример:** Включение "конечного диагноза" в признаки

---

# Как избежать катастрофы? Типичные причины утечки

- Неверное предобработка до разделения на train/test
- Использование будущего для предсказания прошлого
- Дубликаты объектов в train и test
- Утечка через целевой признак

---

# Пошаговая инструкция: Итоговый пайплайн валидации

1. Разделить данные на Train и Test
2. Исследовать и предобрабатывать данные, используя только Train
3. Настроить гиперпараметры с помощью Кросс-валидации на Train
4. Обучить финальную модель на всем Train
5. Один раз оценить на Test set

---

# Что мы выигрываем?

- ✅ Честная оценка качества модели
- ✅ Снижение риска переобучения
- ✅ Уверенность в работе на новых данных
- ✅ Возможность сравнения разных моделей

---

# Чего нужно избегать? Частые ошибки

- Подбор гиперпараметров на тестовом наборе
- Многократное использование тестового набора
- Игнорирование временной структуры данных
- Слепая вера в кросс-валидацию

---

# Ключевые выводы

- Всегда разделяйте данные на train/validation/test
- Используйте кросс-валидацию для надежной оценки
- Тестовый набор используйте строго один раз
- Различайте функцию потерь и метрику
- Переобучение — главный враг, валидация — инструмент борьбы

---

# Вопросы?

**Контакты:** Ваша почта / Telegram

![bg opacity:0.2](https://img.icons8.com/ios/500/000000/qa.png)

---

# Дополнительные материалы

- Scikit-learn documentation: train_test_split, cross_val_score, GridSearchCV
- Книга: "Introduction to Statistical Learning" (ISL)
- Курс: "How to Win a Data Science Competition" на Coursera