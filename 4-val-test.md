---
marp: true
theme: default
style: |
    section {
        font-family: 'Arial', sans-serif;
        font-size: 20px;
    }
paginate: true
header: Валидация и тестирование моделей машинного обучения
footer: БГУ ФПМИ ФМИиС
---

# Валидация и тестирование моделей

## Как оценить, будет ли ваша модель работать в реальном мире

---

# Обобщающая способность (Generalization)

- Конечная цель любой модели — делать точные предсказания на новых, ранее не виденных данных
- Хорошая производительность на обучающих данных — это не достаточное условие
- Ключевой вопрос: Как мы можем оценить эту способность к обобщению до развертывания модели?

---

# Самый простой подход: Hold-Out

- Разделяем весь датасет на две части случайным образом
- Типичное соотношение: 70%/30% или 80%/20%
- **Процесс:**
  1. Обучить модель на Train
  2. Оценить качество на Test
  3. Больше не прикасаться к Test до самого конца!

**Плюсы:** Простота и скорость  
**Минусы:** Оценка зависит от случайного разбиения

---

# Где настраивать гиперпараметры?

- **Hyperparameters**: Параметры, не обучаемые моделью
- **Validation Set**: Отдельная часть данных для подбора гиперпараметров
- Типичное соотношение: 60%/20%/20% или 70%/15%/15%
- **Важно:** Тестовый набор все еще полностью изолирован

---

# Случайное vs Стратифицированное разбиение

- **Случайное разбиение**: Простое случайное перемешивание и разделение
- **Стратифицированное разбиение**: Сохраняет пропорции классов
- **Зачем?** Критически важно для несбалансированных datasets

![bg right:40% w:500](https://miro.medium.com/v2/resize:fit:802/1*hNqNI0JTCIAOU_CszK36Og.png)

---

*Ситуация*
Есть один и тот же тестовый датасет. Есть 2 ML-модели, которые были проскорены на этом датасете со средней метрикой accuracy 25% и 29% соответственно. Как понять, какая модель лучше?

---
*Ответ: это зависит от размера тестового датасета.*

Стоит помнить, что в зависимости от размера датасета можно высчитать доверительный интервал для средней метрики по тестовому набору данных. Это поможет понять, можно ли утверждать об отношении порядка полученных результатов.

Т.е. если у нас получились метрики $25\% \pm 10\%$ и $29\% \pm 12\%$, то ясно, что невозможно точно установить, какая из 2 ML-моделей лучше, а какая хуже

![bg right:50% w:600](https://www.questionpro.com/blog/wp-content/uploads/2022/08/confidence-interval-formula.jpg)

---

# Золотой стандарт оценки: K-Fold Cross-Validation

1. Данные случайно shuffl'ятся и разбиваются на k одинаковых частей
2. Модель обучается k раз
3. Итоговая оценка — среднее по всем k валидационным фолдам

![bg right:40% width:400](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)

---

# Практические аспекты K-Fold CV

- **Выбор K**: Чаще всего k=5 или k=10
- **Stratified K-Fold**: Для задач классификации
- **Для чего используется?**
  - Надежная оценка обобщающей способности
  - Сравнение моделей и выбор лучшей
  - Подбор гиперпараметров

---

# Для временных рядов (Time-Aware CV)

- **Важно:** Запрещено случайное разбиение!
- **Принцип:** Будущее не должно влиять на прошлое
- **Методы:**
  - Rolling Window: Фиксированный размер обучающего окна
  - Expanding Window: Обучающее окно увеличивается

<br>
<br>

*Подробнее в лекции по теме "временные ряды"...*

![bg right:42% w:550](https://forum-cdn.knime.com/uploads/default/original/3X/b/4/b4aa6967ac422881c0b337a7f8de4af897ee6c09.png)


---

# Что оптимизирует алгоритм? Функция потерь

- **Функция потерь (Loss)**: Функция, которую алгоритм минимизирует
- Непрерывная, дифференцируемая (для вычисления градиента)
- **Примеры:** MSE, Log Loss, Huber Loss
- Может не иметь прямой интерпретации для бизнеса

---

# Что действительно интересует бизнес? Метрика качества

- **Метрика (Metric)**: Понятная человеку мера качества модели
- Не обязательно дифференцируема
- **Примеры интерпретируемых метрик:** Accuracy, Precision, Recall, MAE, MAPE, WAPE
- **Примеры неинтерпретируемых метрик**: F1-Score, ROC-AUC, R², RMSE, 
- **Ключевой момент:** Метрика и функция потерь могут не совпадать

---

# Loss vs. Metric

| Критерий | Функция потерь (Loss) | Метрика (Metric) |
|----------|----------------------|------------------|
| Цель | Научить алгоритм | Оценить модель |
| Требования | Должна быть дифференцируема | Должна быть интерпретируема |
| Пример | Log Loss | Precision\Recall |

**Как выбирать метрику?** По бизнес-требованиям и смыслу проекта

---

# Практический кейс несовпадения Loss и Metric

- **Задача:** Обнаружение мошеннических транзакций
- **Функция потерь:** Log Loss (для устойчивого обучения)
- **Метрика:** F1-Score или Precision@Recall=0.9
- **Действия:** Выбираем модель, максимизируя F1-Score

---

# Математическая интерпретация: Bias-Variance Decomposition

Ожидаемая ошибка модели распадается на три части:

- **Смещение (Bias)**: Ошибка из-за слишком простых предположений
- **Разброс (Variance)**: Чувствительность к изменениям в данных
- **Неустранимая ошибка**: Шум в данных

**Bias-Variance Tradeoff:** Нельзя минимизировать и bias, и variance одновременно
![bg vertical right:40% w:500](https://serokell.io/files/y0/y08hu0d1.Bias-Variance_Tradeoff_in_ML_pic6.png)
![bg right:40% w:400](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png)

---

# Что такое переобучение?

- Модель становится слишком сложной и подстраивается под шум и случайные особенности обучающих данных
- Результат: Отличные результаты на тренировочных данных, но плохие на новых
- Модель равносильна студенту, который зазубрил билеты, но не понял тему
- Модель имеет слишком много параметров -> она запоминает каждый элемент выборки вместо понимания общей закономерности
- Результат: Низкая ошибка на training set, высокая на test set

![bg right:50% width:650](https://miro.medium.com/1*_7OPgojau8hkiPUiHoGK_w.png)

---

# Методы борьбы с переобучением

- Сбор большего количества данных
- Упрощение модели
- Регуляризация (L1-Lasso, L2-Ridge)
- Ансамбли (Bagging - Random Forest)
- Ранняя остановка (Early Stopping)

---

# Диагностика переобучения: Learning Curves

- График зависимости ошибки от размера обучающей выборки
- **Переобучение:** Большой разрыв между train и validation error
- **Недообучение:** Обе ошибки высоки и сходятся

![bg right:40% width:530](https://i.sstatic.net/haGpo.png)

---

# Data Leakage (Утечка данных)

- Ситуация, когда в признаки попадает информация, недоступная в реальном прогнозе
- Результат: Слишком оптимистичные оценки, провал в продакшене
- **Пример:** 
  - Модель по предсказанию успеха продаж продукта в магазинах. В данных есть признак `процент_присутствия_на_полке_на_дату`. Он строится следующим образом:

  `запуск товара -> попадание в магазины -> продукт приобретает успешность -> его выставляют чаще на полки -> продажи растут ещё больше`

  - Можно ли использовать такой признак для предсказания неизвестного продукта, который будет запущен в продажу через 2 года?

---

# Temporal Data Leakage

В каком случае легче предсказать недостающие значения?

- Ясно, что просто заполнить пропуски между точками легче, чем вырисовать дальнейшую треакторию в будущем.
- Также проблема в том, что при стратегии выдирания точек прямо из случайных мест ряда мы можем получить какую-то метрику. Но смысл модели не в том, чтобы заполнять пропуски между точек, а предсказывать будущую траекторию!

![bg vertical right:40% w:500](https://i.sstatic.net/uVuVG.png)
![bg vertical right:40% w:500](https://towardsdatascience.com/wp-content/uploads/2021/07/1hCc8qdx4_RDcZdUUlkN98Q-scaled.jpeg)

---

## Запомните пожалуйста раз и навсегда:

# Модель должна тестироваться так, как она будет использоваться.
# Модель должна тестироваться так, как она будет использоваться.
# Модель должна тестироваться так, как она будет использоваться.
# Модель должна тестироваться так, как она будет использоваться.
# Модель должна тестироваться так, как она будет использоваться.
# Модель должна тестироваться так, как она будет использоваться.

*у доски рассказать пример с моделью оттока*

---

# Правильный пайплайн

0. Сформировать Test (golden) set, который будет максимально приближенным к данным, которые будут приходить в реальной жизни. Он должен быть с максимально достоверным таргетом.
1. Набрать данных в Train - здесь уже они не обязательно должны быть чистейшими, концептуально это не запрещается, все зависит от итогового качества
2. На Train части проводить всю настройку модели с K-Fold CV
3. Выбрать лучшую модель
4. Один раз провести финальную оценку на Test set

**Важно:** Кросс-валидация происходит внутри тренировочного множества

---

# Ключевые выводы

- **Модель должна тестироваться так, как будет использоваться**

- Разделяйте данные на train/validation/test
- По возможности используйте кросс-валидацию для надежной оценки
- Тестовый набор используйте строго один раз
- Различайте функцию потерь и метрику
- Переобучение — главный враг, валидация — инструмент борьбы

---

# Дополнительные материалы

- Scikit-learn documentation: train_test_split, cross_val_score, GridSearchCV
- Книга: "Introduction to Statistical Learning" (ISL)
- Курс: "How to Win a Data Science Competition" на Coursera